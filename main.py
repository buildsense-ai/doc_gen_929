#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gauzæ–‡æ¡£Agent - æ™ºèƒ½é•¿æ–‡æ¡£ç”Ÿæˆç³»ç»Ÿ
ä¸»ç¨‹åºå…¥å£

åŸºäºå¤šAgentæ¶æ„çš„æ™ºèƒ½æ–‡æ¡£ç”Ÿæˆç³»ç»Ÿï¼Œæ”¯æŒä»ç”¨æˆ·æŸ¥è¯¢åˆ°å®Œæ•´æ–‡æ¡£çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ç”Ÿæˆã€‚

ç³»ç»Ÿæ¶æ„ï¼š
1. OrchestratorAgent - ç¼–æ’ä»£ç†ï¼šåˆ†æéœ€æ±‚ï¼Œç”Ÿæˆæ–‡æ¡£ç»“æ„å’Œå†™ä½œæŒ‡å¯¼
2. SectionWriterAgent - ç« èŠ‚å†™ä½œä»£ç†ï¼šä½¿ç”¨ReActæ¡†æ¶æ™ºèƒ½æ£€ç´¢ç›¸å…³èµ„æ–™
3. ContentGeneratorAgent - å†…å®¹ç”Ÿæˆä»£ç†ï¼šåŸºäºç»“æ„å’Œèµ„æ–™ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£

ä½¿ç”¨æ–¹æ³•ï¼š
    python main.py [é€‰é¡¹]
    
é€‰é¡¹ï¼š
    --query "æŸ¥è¯¢å†…å®¹"    ç›´æ¥æŒ‡å®šæ–‡æ¡£éœ€æ±‚
    --interactive       è¿›å…¥äº¤äº’æ¨¡å¼
    --help             æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
"""

import sys
import os

# ===== å¿…é¡»åœ¨æ‰€æœ‰å…¶ä»–å¯¼å…¥ä¹‹å‰ç¦ç”¨ChromaDB telemetry =====
os.environ['ANONYMIZED_TELEMETRY'] = 'False'
os.environ['CHROMA_TELEMETRY_DISABLED'] = 'True'

import json
import argparse
import time
from datetime import datetime
from typing import Dict, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from clients.openrouter_client import OpenRouterClient
    # ç§»é™¤SimpleRAGClientå¯¼å…¥
    from Document_Agent.orchestrator_agent import OrchestratorAgent
    from Document_Agent.section_writer_agent import ReactAgent
    from Document_Agent.content_generator_agent import MainDocumentGenerator
    from Document_Agent.final_review_agent import DocumentReviewer
    from Document_Agent.final_review_agent.json_merger import JSONDocumentMerger
    from Document_Agent.final_review_agent.regenerate_sections import DocumentRegenerator
    from config.settings import setup_logging, get_config, get_concurrency_manager
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿æ‚¨åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤ç¨‹åºï¼Œå¹¶å®‰è£…äº†æ‰€æœ‰ä¾èµ–ã€‚")
    sys.exit(1)


class DocumentGenerationPipeline:
    """æ–‡æ¡£ç”Ÿæˆæµæ°´çº¿ - æ•´åˆä¸‰ä¸ªAgentçš„å®Œæ•´å·¥ä½œæµï¼Œæ”¯æŒç»Ÿä¸€å¹¶å‘ç®¡ç†"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµæ°´çº¿"""
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ–‡æ¡£ç”Ÿæˆç³»ç»Ÿ...")
        
        # è®¾ç½®æ—¥å¿—
        setup_logging()
        
        # åˆå§‹åŒ–å¹¶å‘ç®¡ç†å™¨
        self.concurrency_manager = get_concurrency_manager()
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        try:
            self.llm_client = OpenRouterClient()
            # ç§»é™¤rag_clientï¼ŒOrchestratorAgentå·²ç»é›†æˆå¤–éƒ¨API
            
            # åˆå§‹åŒ–äº”ä¸ªAgentï¼Œä¼ å…¥ç»Ÿä¸€çš„å¹¶å‘ç®¡ç†å™¨
            # OrchestratorAgentä¸å†éœ€è¦rag_clientå‚æ•°
            self.orchestrator = OrchestratorAgent(self.llm_client, self.concurrency_manager)
            self.section_writer = ReactAgent(self.llm_client, self.concurrency_manager)
            self.content_generator = MainDocumentGenerator(self.concurrency_manager)
            self.document_reviewer = DocumentReviewer()
            self.document_regenerator = DocumentRegenerator()
            
            print("âœ… ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼ï¼ˆä½¿ç”¨å¤–éƒ¨APIæœåŠ¡ï¼‰")
            self._print_concurrency_settings()
            
        except Exception as e:
            print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _print_concurrency_settings(self):
        """æ‰“å°å½“å‰å¹¶å‘è®¾ç½®"""
        print("\n" + "="*60)
        self.concurrency_manager.print_settings()
        print("="*60 + "\n")
    
    def set_concurrency(self, orchestrator_workers: int = None, react_workers: int = None, 
                       content_workers: int = None, rate_delay: float = None):
        """
        ç»Ÿä¸€è®¾ç½®å¹¶å‘å‚æ•°
        
        Args:
            orchestrator_workers: ç¼–æ’ä»£ç†çº¿ç¨‹æ•°
            react_workers: æ£€ç´¢ä»£ç†çº¿ç¨‹æ•°
            content_workers: å†…å®¹ç”Ÿæˆä»£ç†çº¿ç¨‹æ•°
            rate_delay: è¯·æ±‚é—´éš”æ—¶é—´(ç§’)
        """
        print("ğŸ”§ æ›´æ–°å¹¶å‘è®¾ç½®...")
        
        if orchestrator_workers is not None:
            self.orchestrator.set_max_workers(orchestrator_workers)
            
        if react_workers is not None:
            self.section_writer.set_max_workers(react_workers)
            
        if content_workers is not None:
            self.content_generator.set_max_workers(content_workers)
            
        if rate_delay is not None:
            self.content_generator.set_rate_limit_delay(rate_delay)
            
        print("âœ… å¹¶å‘è®¾ç½®æ›´æ–°å®Œæˆï¼")
        self._print_concurrency_settings()
    
    def get_concurrency_settings(self) -> dict:
        """è·å–å½“å‰å¹¶å‘è®¾ç½®"""
        return {
            'orchestrator_workers': self.orchestrator.get_max_workers(),
            'react_workers': self.section_writer.get_max_workers(),
            'content_workers': self.content_generator.get_max_workers(),
            'rate_delay': self.content_generator.get_rate_limit_delay()
        }
    
    def generate_document(self, user_query: str, project_name: str, output_dir: str = "åŒ»çµå¤åº™") -> Dict[str, str]:
        """
        å®Œæ•´æ–‡æ¡£ç”Ÿæˆæµç¨‹
        
        Args:
            user_query: ç”¨æˆ·éœ€æ±‚æè¿°
            project_name: é¡¹ç›®åç§°ï¼Œç”¨äºRAGæ£€ç´¢
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Dict: åŒ…å«ç”Ÿæˆæ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        print("ğŸš€ å¼€å§‹æ–‡æ¡£ç”Ÿæˆæµç¨‹...")
        print("=" * 80)
        print(f"ğŸ“ ç”¨æˆ·éœ€æ±‚ï¼š{user_query}")
        print(f"ğŸ·ï¸ é¡¹ç›®åç§°ï¼š{project_name}")
        print("=" * 80)
        
        try:
            # é˜¶æ®µ1ï¼šç”Ÿæˆæ–‡æ¡£ç»“æ„ï¼ˆOrchestratorAgentï¼‰
            print("\nğŸ—ï¸  é˜¶æ®µ1ï¼šç”Ÿæˆæ–‡æ¡£ç»“æ„å’Œå†™ä½œæŒ‡å¯¼...")
            step1_start = time.time()
            
            document_guide = self.orchestrator.generate_complete_guide(user_query)
            
            step1_time = time.time() - step1_start
            sections_count = sum(len(part.get('sections', [])) for part in document_guide.get('report_guide', []))
            
            print(f"âœ… æ–‡æ¡£ç»“æ„ç”Ÿæˆå®Œæˆï¼")
            print(f"   ğŸ“Š ç”Ÿæˆäº† {len(document_guide.get('report_guide', []))} ä¸ªä¸»è¦éƒ¨åˆ†ï¼Œ{sections_count} ä¸ªå­ç« èŠ‚")
            print(f"   â±ï¸  è€—æ—¶ï¼š{step1_time:.1f}ç§’")
            
            # ä¿å­˜é˜¶æ®µ1ç»“æœ
            step1_file = os.path.join(output_dir, f"step1_document_guide_{timestamp}.json")
            with open(step1_file, 'w', encoding='utf-8') as f:
                json.dump(document_guide, f, ensure_ascii=False, indent=2)
            
            # é˜¶æ®µ2ï¼šæ™ºèƒ½æ£€ç´¢ç›¸å…³èµ„æ–™ï¼ˆSectionWriterAgentï¼‰
            print("\nğŸ” é˜¶æ®µ2ï¼šä¸ºå„ç« èŠ‚æ™ºèƒ½æ£€ç´¢ç›¸å…³èµ„æ–™...")
            step2_start = time.time()
            
            enriched_guide = self.section_writer.process_report_guide(document_guide, project_name)
            
            step2_time = time.time() - step2_start
            print(f"âœ… èµ„æ–™æ£€ç´¢å®Œæˆï¼")
            print(f"   ğŸ” ä¸º {sections_count} ä¸ªç« èŠ‚æ£€ç´¢äº†ç›¸å…³èµ„æ–™")
            print(f"   â±ï¸  è€—æ—¶ï¼š{step2_time:.1f}ç§’")
            
            # ä¿å­˜é˜¶æ®µ2ç»“æœ
            step2_file = os.path.join(output_dir, f"step2_enriched_guide_{timestamp}.json")
            with open(step2_file, 'w', encoding='utf-8') as f:
                json.dump(enriched_guide, f, ensure_ascii=False, indent=2)
            
            # é˜¶æ®µ3ï¼šç”Ÿæˆæœ€ç»ˆæ–‡æ¡£ï¼ˆContentGeneratorAgentï¼‰
            print("\nğŸ“ é˜¶æ®µ3ï¼šç”Ÿæˆæœ€ç»ˆæ–‡æ¡£å†…å®¹...")
            step3_start = time.time()
            
            # ä¿å­˜ä¸ºcontent_generatorèƒ½è¯†åˆ«çš„æ–‡ä»¶å
            generation_input = os.path.join(output_dir, f"ç”Ÿæˆæ–‡æ¡£çš„ä¾æ®_{timestamp}.json")
            with open(generation_input, 'w', encoding='utf-8') as f:
                json.dump(enriched_guide, f, ensure_ascii=False, indent=2)
            
            # ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£
            final_doc_path = self.content_generator.generate_document(generation_input)
            
            step3_time = time.time() - step3_start
            print(f"âœ… æœ€ç»ˆæ–‡æ¡£ç”Ÿæˆå®Œæˆï¼")
            print(f"   â±ï¸  è€—æ—¶ï¼š{step3_time:.1f}ç§’")
            
            # é˜¶æ®µ4ï¼šæ–‡æ¡£è´¨é‡è¯„ä¼°ï¼ˆDocumentReviewerï¼‰
            print("\nğŸ“Š é˜¶æ®µ4ï¼šæ–‡æ¡£è´¨é‡è¯„ä¼°...")
            step4_start = time.time()
            
            # è¯»å–ç”Ÿæˆçš„æ–‡æ¡£å†…å®¹
            try:
                with open(final_doc_path, 'r', encoding='utf-8') as f:
                    document_content = f.read()
                
                # è¿›è¡Œè´¨é‡è¯„ä¼°
                document_title = os.path.basename(final_doc_path).replace('.md', '')
                quality_analysis = self.document_reviewer.analyze_document_quality(
                    document_content, document_title
                )
                
                # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
                quality_report = self.document_reviewer.generate_quality_report(
                    quality_analysis, document_title
                )
                
                # ä¿å­˜è´¨é‡åˆ†æç»“æœ
                quality_analysis_file = os.path.join(output_dir, f"quality_analysis_{timestamp}.json")
                self.document_reviewer.save_analysis_result(
                    quality_analysis, document_title, quality_analysis_file
                )
                
                # ä¿å­˜è´¨é‡æŠ¥å‘Š
                quality_report_file = os.path.join(output_dir, f"quality_report_{timestamp}.md")
                with open(quality_report_file, 'w', encoding='utf-8') as f:
                    f.write(quality_report)
                
                step4_time = time.time() - step4_start
                print(f"âœ… æ–‡æ¡£è´¨é‡è¯„ä¼°å®Œæˆï¼")
                print(f"   ğŸ“Š è´¨é‡è¯„åˆ†ï¼š{quality_analysis.overall_quality_score:.2f}/1.00")
                print(f"   âš ï¸  å‘ç°å†—ä½™ï¼š{quality_analysis.total_unnecessary_redundancy_types} ç±»")
                print(f"   â±ï¸  è€—æ—¶ï¼š{step4_time:.1f}ç§’")
                
            except Exception as e:
                print(f"âš ï¸  æ–‡æ¡£è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
                step4_time = 0
                quality_analysis_file = None
                quality_report_file = None
            
            # è®¡ç®—æ€»è€—æ—¶
            total_time = step1_time + step2_time + step3_time + step4_time
            print("\n" + "=" * 80)
            print("ğŸ‰ æ–‡æ¡£ç”Ÿæˆæµç¨‹å…¨éƒ¨å®Œæˆï¼")
            print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡ï¼š")
            print(f"   ğŸ“‘ ä¸»è¦éƒ¨åˆ†ï¼š{len(document_guide.get('report_guide', []))} ä¸ª")
            print(f"   ğŸ“„ å­ç« èŠ‚ï¼š{sections_count} ä¸ª")
            if step4_time > 0:
                print(f"   ğŸ“Š è´¨é‡è¯„åˆ†ï¼š{quality_analysis.overall_quality_score:.2f}/1.00")
            print(f"   â±ï¸  æ€»è€—æ—¶ï¼š{total_time:.1f}ç§’")
            print("=" * 80)
            
            # è¿”å›ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„
            result = {
                'document_guide': step1_file,
                'enriched_guide': step2_file,
                'generation_input': generation_input,
                'final_document': final_doc_path,
                'output_directory': output_dir
            }
            
            # æ·»åŠ è´¨é‡è¯„ä¼°æ–‡ä»¶ï¼ˆå¦‚æœç”ŸæˆæˆåŠŸï¼‰
            if step4_time > 0:
                result['quality_analysis'] = quality_analysis_file
                result['quality_report'] = quality_report_file
            
            return result
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
    
    def regenerate_and_merge_document(self, original_json_path: str, quality_analysis_path: str, 
                                    output_dir: str = None) -> Dict[str, str]:
        """
        åŸºäºè´¨é‡è¯„ä¼°ç»“æœé‡æ–°ç”Ÿæˆå¹¶åˆå¹¶æ–‡æ¡£
        
        Args:
            original_json_path: åŸå§‹JSONæ–‡æ¡£è·¯å¾„
            quality_analysis_path: è´¨é‡è¯„ä¼°ç»“æœè·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Dict: åŒ…å«ç”Ÿæˆæ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        if output_dir is None:
            output_dir = os.path.dirname(original_json_path)
        
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        print("ğŸ”„ å¼€å§‹æ–‡æ¡£é‡æ–°ç”Ÿæˆå’Œåˆå¹¶æµç¨‹...")
        print("=" * 80)
        print(f"ğŸ“„ åŸå§‹æ–‡æ¡£ï¼š{original_json_path}")
        print(f"ğŸ“Š è´¨é‡è¯„ä¼°ï¼š{quality_analysis_path}")
        print("=" * 80)
        
        try:
            # é˜¶æ®µ1ï¼šé‡æ–°ç”Ÿæˆéœ€è¦ä¿®æ”¹çš„ç« èŠ‚
            print("\nğŸ”§ é˜¶æ®µ1ï¼šé‡æ–°ç”Ÿæˆéœ€è¦ä¿®æ”¹çš„ç« èŠ‚...")
            step1_start = time.time()
            
            regenerated_sections = self.document_regenerator.regenerate_document_sections(
                quality_analysis_path, original_json_path, output_dir
            )
            
            # regenerate_document_sectionsè¿”å›çš„æ˜¯å­—å…¸ï¼Œéœ€è¦ä¿å­˜ä¸ºæ–‡ä»¶
            regenerated_sections_path = os.path.join(output_dir, f"regenerated_sections_{timestamp}.json")
            with open(regenerated_sections_path, 'w', encoding='utf-8') as f:
                json.dump(regenerated_sections, f, ensure_ascii=False, indent=2)
            
            step1_time = time.time() - step1_start
            print(f"âœ… ç« èŠ‚é‡æ–°ç”Ÿæˆå®Œæˆï¼")
            print(f"   â±ï¸  è€—æ—¶ï¼š{step1_time:.1f}ç§’")
            
            # é˜¶æ®µ2ï¼šåˆå¹¶æ–‡æ¡£
            print("\nğŸ”— é˜¶æ®µ2ï¼šåˆå¹¶é‡æ–°ç”Ÿæˆçš„ç« èŠ‚...")
            step2_start = time.time()
            
            merger = JSONDocumentMerger(original_json_path, regenerated_sections_path)
            merger.load_original_json()
            merger.load_regenerated_sections()
            
            # åˆå¹¶JSONæ–‡æ¡£
            merged_data = merger.merge_json_documents()
            
            # ä¿å­˜åˆå¹¶åçš„JSON
            merged_json_path = os.path.join(output_dir, f"merged_document_{timestamp}.json")
            merger.save_merged_json(merged_data, merged_json_path)
            
            # è½¬æ¢ä¸ºMarkdown
            merged_md_path = os.path.join(output_dir, f"merged_document_{timestamp}.md")
            merger.convert_to_markdown(merged_data, merged_md_path)
            
            # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
            merger.generate_summary_report(merged_json_path, merged_md_path)
            
            step2_time = time.time() - step2_start
            print(f"âœ… æ–‡æ¡£åˆå¹¶å®Œæˆï¼")
            print(f"   â±ï¸  è€—æ—¶ï¼š{step2_time:.1f}ç§’")
            
            # è®¡ç®—æ€»è€—æ—¶
            total_time = step1_time + step2_time
            print("\n" + "=" * 80)
            print("ğŸ‰ æ–‡æ¡£é‡æ–°ç”Ÿæˆå’Œåˆå¹¶æµç¨‹å®Œæˆï¼")
            print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡ï¼š")
            print(f"   â±ï¸  æ€»è€—æ—¶ï¼š{total_time:.1f}ç§’")
            print("=" * 80)
            
            # è¿”å›ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„
            result = {
                'regenerated_sections': regenerated_sections_path,
                'merged_json': merged_json_path,
                'merged_document': merged_md_path,
                'output_directory': output_dir
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£é‡æ–°ç”Ÿæˆå’Œåˆå¹¶è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
    
    def final_review_workflow(self, markdown_file: str, json_file: str, document_title: str, 
                             output_dir: str = None) -> Dict[str, str]:
        """
        æ‰§è¡Œfinal_review_agentå®Œæ•´å·¥ä½œæµç¨‹
        
        Args:
            markdown_file: ç”Ÿæˆçš„markdownæ–‡æ¡£è·¯å¾„
            json_file: åŸå§‹JSONæ–‡æ¡£è·¯å¾„
            document_title: æ–‡æ¡£æ ‡é¢˜
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Dict: åŒ…å«è¯„å®¡å’Œé‡æ–°ç”Ÿæˆç»“æœçš„å­—å…¸
        """
        if output_dir is None:
            output_dir = os.path.dirname(markdown_file)
        
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        print("ğŸ” å¼€å§‹final_review_agentå·¥ä½œæµç¨‹...")
        print("=" * 80)
        print(f"ğŸ“„ Markdownæ–‡æ¡£ï¼š{markdown_file}")
        print(f"ğŸ“Š JSONæ–‡æ¡£ï¼š{json_file}")
        print(f"ğŸ“ æ–‡æ¡£æ ‡é¢˜ï¼š{document_title}")
        print("=" * 80)
        
        try:
            # é˜¶æ®µ1ï¼šæ–‡æ¡£è´¨é‡è¯„å®¡
            print("\nğŸ“‹ é˜¶æ®µ1ï¼šæ‰§è¡Œæ–‡æ¡£è´¨é‡è¯„å®¡...")
            step1_start = time.time()
            
            # è¯»å–æ–‡æ¡£å†…å®¹
            with open(markdown_file, 'r', encoding='utf-8') as f:
                document_content = f.read()
            
            # æ‰§è¡Œç®€åŒ–åˆ†æ
            analysis_result = self.document_reviewer.analyze_document_simple(document_content, markdown_file, document_title)
            
            if not analysis_result:
                print("âŒ æ–‡æ¡£è´¨é‡è¯„å®¡å¤±è´¥")
                return {}
            
            step1_time = time.time() - step1_start
            print(f"âœ… æ–‡æ¡£è´¨é‡è¯„å®¡å®Œæˆï¼")
            print(f"   ğŸ“Š å‘ç°é—®é¢˜ï¼š{len(analysis_result)} ä¸ª")
            print(f"   â±ï¸  è€—æ—¶ï¼š{step1_time:.1f}ç§’")
            
            # ä¿å­˜è¯„å®¡ç»“æœ
            analysis_file = os.path.join(output_dir, f"final_review_analysis_{timestamp}.json")
            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            
            # é˜¶æ®µ2ï¼šæ–‡æ¡£é‡æ–°ç”Ÿæˆ
            print("\nğŸ”„ é˜¶æ®µ2ï¼šæ‰§è¡Œæ–‡æ¡£é‡æ–°ç”Ÿæˆ...")
            step2_start = time.time()
            
            # æ‰§è¡Œé‡æ–°ç”Ÿæˆ
            regeneration_result = self.document_regenerator.regenerate_document_sections(
                analysis_file,
                json_file,
                output_dir=output_dir
            )
            
            if not regeneration_result or regeneration_result.get('error'):
                error_msg = regeneration_result.get('error', 'æœªçŸ¥é”™è¯¯') if regeneration_result else 'è¿”å›ç»“æœä¸ºç©º'
                print(f"âŒ æ–‡æ¡£é‡æ–°ç”Ÿæˆå¤±è´¥: {error_msg}")
                return {'analysis_file': analysis_file}
            
            step2_time = time.time() - step2_start
            print(f"âœ… æ–‡æ¡£é‡æ–°ç”Ÿæˆå®Œæˆï¼")
            print(f"   ğŸ“Š é‡æ–°ç”Ÿæˆç« èŠ‚ï¼š{len(regeneration_result)} ä¸ª")
            print(f"   â±ï¸  è€—æ—¶ï¼š{step2_time:.1f}ç§’")
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_words = sum(result.get('word_count', 0) for result in regeneration_result.values())
            avg_quality = sum(result.get('quality_score', 0) for result in regeneration_result.values()) / len(regeneration_result)
            
            # é˜¶æ®µ3ï¼šç”Ÿæˆå·¥ä½œæµç¨‹æ‘˜è¦
            print("\nğŸ“ˆ é˜¶æ®µ3ï¼šç”Ÿæˆå·¥ä½œæµç¨‹æ‘˜è¦...")
            
            workflow_summary = {
                'timestamp': timestamp,
                'input_files': {
                    'markdown_file': markdown_file,
                    'json_file': json_file,
                    'document_title': document_title
                },
                'analysis_results': {
                    'total_issues': len(analysis_result),
                    'analysis_file': analysis_file
                },
                'regeneration_results': {
                    'total_sections': len(regeneration_result),
                    'total_words': total_words,
                    'average_quality': avg_quality,
                    'output_directory': output_dir
                },
                'status': 'success'
            }
            
            # ä¿å­˜å·¥ä½œæµç¨‹æ‘˜è¦
            summary_file = os.path.join(output_dir, f"final_review_summary_{timestamp}.json")
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(workflow_summary, f, ensure_ascii=False, indent=2)
            
            # è®¡ç®—æ€»è€—æ—¶
            total_time = step1_time + step2_time
            print("\n" + "=" * 80)
            print("ğŸ‰ final_review_agentå·¥ä½œæµç¨‹å®Œæˆï¼")
            print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡ï¼š")
            print(f"   ğŸ“‹ å‘ç°é—®é¢˜ï¼š{len(analysis_result)} ä¸ª")
            print(f"   ğŸ“ é‡æ–°ç”Ÿæˆç« èŠ‚ï¼š{len(regeneration_result)} ä¸ª")
            print(f"   ğŸ“„ æ€»å­—æ•°ï¼š{total_words} å­—")
            print(f"   ğŸ“Š å¹³å‡è´¨é‡ï¼š{avg_quality:.2f}")
            print(f"   â±ï¸  æ€»è€—æ—¶ï¼š{total_time:.1f}ç§’")
            print("=" * 80)
            
            # è¿”å›ç»“æœ
            result = {
                'analysis_file': analysis_file,
                'summary_file': summary_file,
                'output_directory': output_dir,
                'regeneration_sections': len(regeneration_result),
                'total_words': total_words,
                'average_quality': avg_quality
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ final_review_agentå·¥ä½œæµç¨‹å¤±è´¥: {e}")
            raise
    
    def complete_workflow_with_regeneration(self, user_query: str, project_name: str = "é»˜è®¤é¡¹ç›®", 
                                          output_dir: str = "outputs", auto_regenerate: bool = True) -> Dict[str, str]:
        """
        å®Œæ•´çš„æ–‡æ¡£ç”Ÿæˆå·¥ä½œæµï¼ŒåŒ…å«è´¨é‡è¯„ä¼°å’Œè‡ªåŠ¨é‡æ–°ç”Ÿæˆ
        
        Args:
            user_query: ç”¨æˆ·éœ€æ±‚æè¿°
            project_name: é¡¹ç›®åç§°ï¼Œç”¨äºRAGæ£€ç´¢
            output_dir: è¾“å‡ºç›®å½•
            auto_regenerate: æ˜¯å¦è‡ªåŠ¨é‡æ–°ç”Ÿæˆä½è´¨é‡ç« èŠ‚
            
        Returns:
            Dict: åŒ…å«ç”Ÿæˆæ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        # é¦–å…ˆæ‰§è¡Œæ ‡å‡†çš„æ–‡æ¡£ç”Ÿæˆæµç¨‹
        initial_result = self.generate_document(user_query, project_name, output_dir)
        
        if not auto_regenerate:
            return initial_result
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è´¨é‡è¯„ä¼°ç»“æœ
        if 'quality_analysis' not in initial_result:
            print("âš ï¸  æœªæ‰¾åˆ°è´¨é‡è¯„ä¼°ç»“æœï¼Œè·³è¿‡è‡ªåŠ¨é‡æ–°ç”Ÿæˆ")
            return initial_result
        
        # æ£€æŸ¥æ˜¯å¦æœ‰éœ€è¦ä¿®æ”¹çš„ç« èŠ‚
        try:
            with open(initial_result['quality_analysis'], 'r', encoding='utf-8') as f:
                quality_data = json.load(f)
            
            # æ£€æŸ¥å†—ä½™åˆ†æç»“æœ
            redundancy_count = quality_data.get('total_unnecessary_redundancy_types', 0)
            redundancy_analysis = quality_data.get('unnecessary_redundancies_analysis', [])
            quality_score = quality_data.get('overall_quality_score', 1.0)
            
            # è®¾ç½®é‡æ–°ç”Ÿæˆçš„é˜ˆå€¼ï¼šå†—ä½™ç±»å‹è¶…è¿‡3ä¸ªæˆ–è´¨é‡åˆ†æ•°ä½äº0.7
            should_regenerate = redundancy_count > 3 or quality_score < 0.7
            
            if not should_regenerate:
                print(f"âœ… æ–‡æ¡£è´¨é‡è‰¯å¥½ï¼ˆå†—ä½™ç±»å‹: {redundancy_count}, è´¨é‡åˆ†: {quality_score:.2f}ï¼‰ï¼Œæ— éœ€é‡æ–°ç”Ÿæˆ")
                return initial_result
            
            print(f"âš ï¸  æ–‡æ¡£è´¨é‡éœ€è¦æ”¹è¿›ï¼ˆå†—ä½™ç±»å‹: {redundancy_count}, è´¨é‡åˆ†: {quality_score:.2f}ï¼‰ï¼Œå¼€å§‹è‡ªåŠ¨é‡æ–°ç”Ÿæˆ...")
            
            # æ‰§è¡Œé‡æ–°ç”Ÿæˆå’Œåˆå¹¶
            regeneration_result = self.regenerate_and_merge_document(
                initial_result['final_document'],  # ä¼ é€’æœ€ç»ˆç”Ÿæˆçš„JSONæ–‡æ¡£
                initial_result['quality_analysis'],  # ä¼ é€’è´¨é‡åˆ†ææ–‡ä»¶
                output_dir
            )
            
            # åˆå¹¶ç»“æœ
            final_result = {**initial_result, **regeneration_result}
            final_result['final_document'] = regeneration_result['merged_document']
            
            return final_result
            
        except Exception as e:
            print(f"âš ï¸  è‡ªåŠ¨é‡æ–°ç”Ÿæˆå¤±è´¥: {e}")
            print("ğŸ“„ è¿”å›åˆå§‹ç”Ÿæˆç»“æœ")
            return initial_result
    
    def generate_document_without_evaluation(self, user_query: str, project_name: str = "é»˜è®¤é¡¹ç›®", output_dir: str = "outputs") -> Dict[str, str]:
        """
        å®Œæ•´æ–‡æ¡£ç”Ÿæˆæµç¨‹ï¼ˆä¸åŒ…å«è´¨é‡è¯„ä¼°é˜¶æ®µï¼‰
        ä¸“ä¸ºAPIæœåŠ¡å™¨è®¾è®¡ï¼Œè·³è¿‡è´¨é‡è¯„ä¼°ä»¥æé«˜å“åº”é€Ÿåº¦
        
        Args:
            user_query: ç”¨æˆ·éœ€æ±‚æè¿°
            project_name: é¡¹ç›®åç§°ï¼Œç”¨äºRAGæ£€ç´¢
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Dict: åŒ…å«ç”Ÿæˆæ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        print("ğŸš€ å¼€å§‹æ–‡æ¡£ç”Ÿæˆæµç¨‹...")
        print("=" * 80)
        print(f"ğŸ“ ç”¨æˆ·éœ€æ±‚ï¼š{user_query}")
        print(f"ğŸ·ï¸ é¡¹ç›®åç§°ï¼š{project_name}")
        print("=" * 80)
        
        try:
            # é˜¶æ®µ1ï¼šç”Ÿæˆæ–‡æ¡£ç»“æ„ï¼ˆOrchestratorAgentï¼‰
            print("\nğŸ—ï¸  é˜¶æ®µ1ï¼šç”Ÿæˆæ–‡æ¡£ç»“æ„å’Œå†™ä½œæŒ‡å¯¼...")
            step1_start = time.time()
            
            document_guide = self.orchestrator.generate_complete_guide(user_query)
            
            step1_time = time.time() - step1_start
            sections_count = sum(len(part.get('sections', [])) for part in document_guide.get('report_guide', []))
            
            print(f"âœ… æ–‡æ¡£ç»“æ„ç”Ÿæˆå®Œæˆï¼")
            print(f"   ğŸ“Š ç”Ÿæˆäº† {len(document_guide.get('report_guide', []))} ä¸ªä¸»è¦éƒ¨åˆ†ï¼Œ{sections_count} ä¸ªå­ç« èŠ‚")
            print(f"   â±ï¸  è€—æ—¶ï¼š{step1_time:.1f}ç§’")
            
            # ä¿å­˜é˜¶æ®µ1ç»“æœ
            step1_file = os.path.join(output_dir, f"step1_document_guide_{timestamp}.json")
            with open(step1_file, 'w', encoding='utf-8') as f:
                json.dump(document_guide, f, ensure_ascii=False, indent=2)
            
            # é˜¶æ®µ2ï¼šæ™ºèƒ½æ£€ç´¢ç›¸å…³èµ„æ–™ï¼ˆSectionWriterAgentï¼‰
            print("\nğŸ” é˜¶æ®µ2ï¼šä¸ºå„ç« èŠ‚æ™ºèƒ½æ£€ç´¢ç›¸å…³èµ„æ–™...")
            step2_start = time.time()
            
            enriched_guide = self.section_writer.process_report_guide(document_guide, project_name)
            
            step2_time = time.time() - step2_start
            print(f"âœ… èµ„æ–™æ£€ç´¢å®Œæˆï¼")
            print(f"   ğŸ” ä¸º {sections_count} ä¸ªç« èŠ‚æ£€ç´¢äº†ç›¸å…³èµ„æ–™")
            print(f"   â±ï¸  è€—æ—¶ï¼š{step2_time:.1f}ç§’")
            
            # ä¿å­˜é˜¶æ®µ2ç»“æœ
            step2_file = os.path.join(output_dir, f"step2_enriched_guide_{timestamp}.json")
            with open(step2_file, 'w', encoding='utf-8') as f:
                json.dump(enriched_guide, f, ensure_ascii=False, indent=2)
            
            # é˜¶æ®µ3ï¼šç”Ÿæˆæœ€ç»ˆæ–‡æ¡£ï¼ˆContentGeneratorAgentï¼‰
            print("\nğŸ“ é˜¶æ®µ3ï¼šç”Ÿæˆæœ€ç»ˆæ–‡æ¡£å†…å®¹...")
            step3_start = time.time()
            
            # ä¿å­˜ä¸ºcontent_generatorèƒ½è¯†åˆ«çš„æ–‡ä»¶å
            generation_input = os.path.join(output_dir, f"ç”Ÿæˆæ–‡æ¡£çš„ä¾æ®_{timestamp}.json")
            with open(generation_input, 'w', encoding='utf-8') as f:
                json.dump(enriched_guide, f, ensure_ascii=False, indent=2)
            
            # ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£
            final_doc_path = self.content_generator.generate_document(generation_input)
            
            step3_time = time.time() - step3_start
            print(f"âœ… æœ€ç»ˆæ–‡æ¡£ç”Ÿæˆå®Œæˆï¼")
            print(f"   â±ï¸  è€—æ—¶ï¼š{step3_time:.1f}ç§’")
            
            # è®¡ç®—æ€»è€—æ—¶ï¼ˆä¸åŒ…å«è´¨é‡è¯„ä¼°ï¼‰
            total_time = step1_time + step2_time + step3_time
            print("\n" + "=" * 80)
            print("ğŸ‰ æ–‡æ¡£ç”Ÿæˆæµç¨‹å®Œæˆï¼ï¼ˆå·²è·³è¿‡è´¨é‡è¯„ä¼°ï¼‰")
            print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡ï¼š")
            print(f"   ğŸ“‘ ä¸»è¦éƒ¨åˆ†ï¼š{len(document_guide.get('report_guide', []))} ä¸ª")
            print(f"   ğŸ“„ å­ç« èŠ‚ï¼š{sections_count} ä¸ª")
            print(f"   â±ï¸  æ€»è€—æ—¶ï¼š{total_time:.1f}ç§’")
            print("=" * 80)
            
            # è¿”å›ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„ï¼ˆä¸åŒ…å«è´¨é‡è¯„ä¼°æ–‡ä»¶ï¼‰
            result = {
                'document_guide': step1_file,
                'enriched_guide': step2_file,
                'generation_input': generation_input,
                'final_document': final_doc_path,
                'output_directory': output_dir
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise


def print_banner():
    """æ‰“å°ç¨‹åºæ¨ªå¹…"""
    banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        Gauzæ–‡æ¡£Agent - æ™ºèƒ½é•¿æ–‡æ¡£ç”Ÿæˆç³»ç»Ÿ                        â•‘
â•‘                                                                              â•‘
â•‘  ğŸ¤– åŸºäºå¤šAgentæ¶æ„çš„æ™ºèƒ½æ–‡æ¡£ç”Ÿæˆç³»ç»Ÿ                                            â•‘
â•‘  ğŸ“ æ”¯æŒä»æŸ¥è¯¢åˆ°å®Œæ•´æ–‡æ¡£çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ç”Ÿæˆ                                        â•‘
â•‘  ğŸš€ é›†æˆç»“æ„è§„åˆ’ã€æ™ºèƒ½æ£€ç´¢ã€å†…å®¹ç”Ÿæˆã€è´¨é‡è¯„ä¼°å››å¤§æ ¸å¿ƒåŠŸèƒ½                        â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(banner)


def interactive_mode():
    """äº¤äº’æ¨¡å¼"""
    print("\nğŸ® è¿›å…¥äº¤äº’æ¨¡å¼")
    print("ğŸ’¡ æ‚¨å¯ä»¥è¾“å…¥ä»»ä½•æ–‡æ¡£éœ€æ±‚ï¼Œç³»ç»Ÿå°†ä¸ºæ‚¨è‡ªåŠ¨ç”Ÿæˆå®Œæ•´çš„ä¸“ä¸šæ–‡æ¡£")
    print("ğŸ“Œ æ”¯æŒçš„æ–‡æ¡£ç±»å‹ï¼šè¯„ä¼°æŠ¥å‘Šã€åˆ†ææŠ¥å‘Šã€æ–¹æ¡ˆä¹¦ã€æŠ€æœ¯æ–‡æ¡£ç­‰")
    print("ğŸ”„ æ”¯æŒæ™ºèƒ½é‡æ–°ç”Ÿæˆï¼šåŸºäºè´¨é‡è¯„ä¼°è‡ªåŠ¨ä¼˜åŒ–æ–‡æ¡£")
    print("âš¡ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç¨‹åº")
    print("âš¡ è¾“å…¥ 'regenerate' è¿›å…¥æ–‡æ¡£é‡æ–°ç”Ÿæˆæ¨¡å¼")
    print("âš¡ è¾“å…¥ 'final_review' è¿›å…¥final_review_agentæ¨¡å¼")
    
    pipeline = DocumentGenerationPipeline()
    
    while True:
        print("\n" + "-" * 60)
        user_input = input("ğŸ“ è¯·æè¿°æ‚¨éœ€è¦ç”Ÿæˆçš„æ–‡æ¡£ï¼š").strip()
        
        if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
            print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨Gauzæ–‡æ¡£Agentï¼Œå†è§ï¼")
            break
            
        if not user_input:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ–‡æ¡£æè¿°")
            continue
        
        if user_input.lower() == 'regenerate':
            # è¿›å…¥æ–‡æ¡£é‡æ–°ç”Ÿæˆæ¨¡å¼
            print("\nğŸ”„ è¿›å…¥æ–‡æ¡£é‡æ–°ç”Ÿæˆæ¨¡å¼")
            original_json = input("ğŸ“„ è¯·è¾“å…¥åŸå§‹JSONæ–‡æ¡£è·¯å¾„ï¼š").strip()
            quality_analysis = input("ğŸ“Š è¯·è¾“å…¥è´¨é‡è¯„ä¼°æ–‡ä»¶è·¯å¾„ï¼š").strip()
            
            if not original_json or not quality_analysis:
                print("âŒ è¯·æä¾›æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„")
                continue
            
            try:
                result_files = pipeline.regenerate_and_merge_document(
                    original_json, quality_analysis
                )
                
                print(f"\nğŸ“ é‡æ–°ç”Ÿæˆçš„æ–‡ä»¶ï¼š")
                print(f"   ğŸ“„ åˆå¹¶åæ–‡æ¡£: {result_files['merged_document']}")
                print(f"   ğŸ“Š é‡æ–°ç”Ÿæˆçš„ç« èŠ‚: {result_files['regenerated_sections']}")
                print(f"   ğŸ“‹ åˆå¹¶åJSON: {result_files['merged_json']}")
                
                print(f"\nâœ¨ æ‚¨å¯ä»¥åœ¨ '{result_files['output_directory']}' ç›®å½•ä¸‹æŸ¥çœ‹æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶")
                
            except Exception as e:
                print(f"âŒ é‡æ–°ç”Ÿæˆå¤±è´¥: {e}")
                print("ğŸ’¡ è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
            
            continue
        
        if user_input.lower() == 'final_review':
            # è¿›å…¥final_review_agentæ¨¡å¼
            print("\nğŸ” è¿›å…¥final_review_agentæ¨¡å¼")
            markdown_file = input("ğŸ“„ è¯·è¾“å…¥Markdownæ–‡æ¡£è·¯å¾„ï¼š").strip()
            json_file = input("ğŸ“Š è¯·è¾“å…¥åŸå§‹JSONæ–‡æ¡£è·¯å¾„ï¼š").strip()
            document_title = input("ğŸ“ è¯·è¾“å…¥æ–‡æ¡£æ ‡é¢˜ï¼š").strip()
            
            if not markdown_file or not json_file or not document_title:
                print("âŒ è¯·æä¾›æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„å’Œæ–‡æ¡£æ ‡é¢˜")
                continue
            
            try:
                result_files = pipeline.final_review_workflow(
                    markdown_file, json_file, document_title
                )
                
                print(f"\nğŸ“ final_review_agentç»“æœï¼š")
                print(f"   ğŸ“‹ è¯„å®¡ç»“æœ: {result_files.get('analysis_file', 'N/A')}")
                print(f"   ğŸ“Š å·¥ä½œæµç¨‹æ‘˜è¦: {result_files.get('summary_file', 'N/A')}")
                print(f"   ğŸ“ é‡æ–°ç”Ÿæˆç« èŠ‚: {result_files.get('regeneration_sections', 0)} ä¸ª")
                print(f"   ğŸ“„ æ€»å­—æ•°: {result_files.get('total_words', 0)} å­—")
                print(f"   ğŸ“Š å¹³å‡è´¨é‡: {result_files.get('average_quality', 0):.2f}")
                
                print(f"\nâœ¨ æ‚¨å¯ä»¥åœ¨ '{result_files['output_directory']}' ç›®å½•ä¸‹æŸ¥çœ‹æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶")
                
            except Exception as e:
                print(f"âŒ final_review_agentå¤±è´¥: {e}")
                print("ğŸ’¡ è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
            
            continue
        
        try:
            # è¯¢é—®æ˜¯å¦å¯ç”¨è‡ªåŠ¨é‡æ–°ç”Ÿæˆ
            auto_regen = input("ğŸ”„ æ˜¯å¦å¯ç”¨è‡ªåŠ¨é‡æ–°ç”ŸæˆåŠŸèƒ½ï¼Ÿ(y/N): ").strip().lower()
            use_regeneration = auto_regen in ['y', 'yes', 'æ˜¯', 'å¯ç”¨']
            
            if use_regeneration:
                # ä½¿ç”¨å®Œæ•´å·¥ä½œæµï¼ˆåŒ…å«è‡ªåŠ¨é‡æ–°ç”Ÿæˆï¼‰
                result_files = pipeline.complete_workflow_with_regeneration(
                    user_input, "åŒ»çµå¤åº™", "åŒ»çµå¤åº™", auto_regenerate=True
                )
            else:
                # ä½¿ç”¨æ ‡å‡†å·¥ä½œæµ
                result_files = pipeline.generate_document(user_input, "åŒ»çµå¤åº™")
            
            print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶ï¼š")
            for file_type, file_path in result_files.items():
                if file_type != 'output_directory':
                    if file_type == 'final_document':
                        print(f"   ğŸ“„ æœ€ç»ˆæ–‡æ¡£: {file_path}")
                    elif file_type == 'quality_analysis':
                        print(f"   ğŸ“Š è´¨é‡åˆ†æ: {file_path}")
                    elif file_type == 'quality_report':
                        print(f"   ğŸ“‹ è´¨é‡æŠ¥å‘Š: {file_path}")
                    elif file_type == 'merged_document':
                        print(f"   ğŸ”„ é‡æ–°ç”Ÿæˆåæ–‡æ¡£: {file_path}")
                    elif file_type == 'regenerated_sections':
                        print(f"   ğŸ“ é‡æ–°ç”Ÿæˆçš„ç« èŠ‚: {file_path}")
                    else:
                        print(f"   {file_type}: {file_path}")
            
            print(f"\nâœ¨ æ‚¨å¯ä»¥åœ¨ '{result_files['output_directory']}' ç›®å½•ä¸‹æŸ¥çœ‹æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            print("ğŸ’¡ è¯·å°è¯•é‡æ–°æè¿°æ‚¨çš„éœ€æ±‚æˆ–æ£€æŸ¥ç³»ç»Ÿé…ç½®")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Gauzæ–‡æ¡£Agent - æ™ºèƒ½é•¿æ–‡æ¡£ç”Ÿæˆç³»ç»Ÿ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python main.py --interactive
  python main.py --query "ä¸ºåŸå¸‚æ›´æ–°é¡¹ç›®ç¼–å†™ç¯å¢ƒå½±å“è¯„ä¼°æŠ¥å‘Š"
  python main.py --query "ç™½äº‘åŒºæ–‡ç‰©ä¿æŠ¤å½±å“è¯„ä¼°æŠ¥å‘Š" --output outputs/heritage
        """
    )
    
    parser.add_argument(
        '--query', '-q',
        type=str,
        help='ç›´æ¥æŒ‡å®šæ–‡æ¡£ç”Ÿæˆéœ€æ±‚'
    )
    
    parser.add_argument(
        '--interactive', '-i',
        action='store_true',
        help='è¿›å…¥äº¤äº’æ¨¡å¼'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        default='åŒ»çµå¤åº™',
        help='æŒ‡å®šè¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šåŒ»çµå¤åº™ï¼‰'
    )
    
    parser.add_argument(
        '--regenerate', '-r',
        action='store_true',
        help='å¯ç”¨è‡ªåŠ¨é‡æ–°ç”ŸæˆåŠŸèƒ½ï¼ˆåŸºäºè´¨é‡è¯„ä¼°ç»“æœï¼‰'
    )
    
    parser.add_argument(
        '--merge-only',
        nargs=2,
        metavar=('ORIGINAL_JSON', 'QUALITY_ANALYSIS'),
        help='ä»…æ‰§è¡Œæ–‡æ¡£é‡æ–°ç”Ÿæˆå’Œåˆå¹¶ï¼ˆéœ€è¦æä¾›åŸå§‹JSONæ–‡æ¡£å’Œè´¨é‡è¯„ä¼°æ–‡ä»¶è·¯å¾„ï¼‰'
    )
    
    parser.add_argument(
        '--final-review',
        nargs=3,
        metavar=('MARKDOWN_FILE', 'JSON_FILE', 'DOCUMENT_TITLE'),
        help='æ‰§è¡Œfinal_review_agentå·¥ä½œæµç¨‹ï¼ˆéœ€è¦æä¾›Markdownæ–‡æ¡£ã€JSONæ–‡æ¡£å’Œæ–‡æ¡£æ ‡é¢˜ï¼‰'
    )
    
    args = parser.parse_args()
    
    # æ‰“å°æ¨ªå¹…
    print_banner()
    
    # æ£€æŸ¥å‚æ•°
    if not args.query and not args.interactive and not args.merge_only and not args.final_review:
        print("ğŸ’¡ è¯·ä½¿ç”¨ --query æŒ‡å®šéœ€æ±‚æˆ–ä½¿ç”¨ --interactive è¿›å…¥äº¤äº’æ¨¡å¼")
        print("ğŸ“– ä½¿ç”¨ --help æŸ¥çœ‹è¯¦ç»†å¸®åŠ©ä¿¡æ¯")
        return
    
    try:
        pipeline = DocumentGenerationPipeline()
        
        if args.merge_only:
            # ä»…æ‰§è¡Œé‡æ–°ç”Ÿæˆå’Œåˆå¹¶æ¨¡å¼
            print(f"ğŸ”„ æ–‡æ¡£é‡æ–°ç”Ÿæˆå’Œåˆå¹¶æ¨¡å¼")
            original_json, quality_analysis = args.merge_only
            result_files = pipeline.regenerate_and_merge_document(
                original_json, quality_analysis, args.output
            )
            
            print(f"\nğŸ“ æ–‡æ¡£å·²é‡æ–°ç”Ÿæˆåˆ°ç›®å½•ï¼š{result_files['output_directory']}")
            print(f"ğŸ“„ åˆå¹¶åæ–‡æ¡£ï¼š{result_files['merged_document']}")
            print(f"ğŸ“Š é‡æ–°ç”Ÿæˆçš„ç« èŠ‚ï¼š{result_files['regenerated_sections']}")
            
        elif args.final_review:
            # æ‰§è¡Œfinal_review_agentæ¨¡å¼
            print(f"ğŸ” final_review_agentæ¨¡å¼")
            markdown_file, json_file, document_title = args.final_review
            result_files = pipeline.final_review_workflow(
                markdown_file, json_file, document_title, args.output
            )
            
            print(f"\nğŸ“ final_review_agentå·²å®Œæˆåˆ°ç›®å½•ï¼š{result_files['output_directory']}")
            print(f"ğŸ“‹ è¯„å®¡ç»“æœï¼š{result_files['analysis_file']}")
            print(f"ğŸ“Š å·¥ä½œæµç¨‹æ‘˜è¦ï¼š{result_files['summary_file']}")
            print(f"ğŸ“ é‡æ–°ç”Ÿæˆç« èŠ‚ï¼š{result_files['regeneration_sections']} ä¸ª")
            print(f"ğŸ“„ æ€»å­—æ•°ï¼š{result_files['total_words']} å­—")
            print(f"ğŸ“Š å¹³å‡è´¨é‡ï¼š{result_files['average_quality']:.2f}")
            
        elif args.interactive:
            # äº¤äº’æ¨¡å¼
            interactive_mode()
        else:
            # ç›´æ¥ç”Ÿæˆæ¨¡å¼
            print(f"ğŸ¯ ç›´æ¥ç”Ÿæˆæ¨¡å¼")
            
            if args.regenerate:
                # ä½¿ç”¨å®Œæ•´å·¥ä½œæµï¼ˆåŒ…å«è‡ªåŠ¨é‡æ–°ç”Ÿæˆï¼‰
                result_files = pipeline.complete_workflow_with_regeneration(
                    args.query, "åŒ»çµå¤åº™", args.output, auto_regenerate=True
                )
            else:
                # ä½¿ç”¨æ ‡å‡†å·¥ä½œæµ
                result_files = pipeline.generate_document(args.query, "åŒ»çµå¤åº™", args.output)
            
            print(f"\nğŸ“ æ–‡æ¡£å·²ç”Ÿæˆåˆ°ç›®å½•ï¼š{result_files['output_directory']}")
            print(f"ğŸ“„ æœ€ç»ˆæ–‡æ¡£ï¼š{result_files['final_document']}")
            if 'quality_report' in result_files:
                print(f"ğŸ“Š è´¨é‡æŠ¥å‘Šï¼š{result_files['quality_report']}")
                print(f"ğŸ“‹ è´¨é‡åˆ†æï¼š{result_files['quality_analysis']}")
            if 'merged_document' in result_files:
                print(f"ğŸ”„ é‡æ–°ç”Ÿæˆåæ–‡æ¡£ï¼š{result_files['merged_document']}")
                print(f"ğŸ“ é‡æ–°ç”Ÿæˆçš„ç« èŠ‚ï¼š{result_files['regenerated_sections']}")
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
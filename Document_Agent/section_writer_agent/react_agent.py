"""
ReAct Agent - æ™ºèƒ½é€Ÿç‡æ§åˆ¶å¢å¼ºç‰ˆ

æ­¤ç‰ˆæœ¬å°†å¹¶è¡Œå¤„ç†é€»è¾‘å°è£…åœ¨Agentå†…éƒ¨ï¼Œè°ƒç”¨æ–¹åªéœ€è°ƒç”¨ä¸€ä¸ªæ–¹æ³•å³å¯å¤„ç†æ•´ä¸ªæŠ¥å‘Šã€‚
é›†æˆæ™ºèƒ½é€Ÿç‡æ§åˆ¶ç³»ç»Ÿï¼Œå®ç°æ›´é«˜æ•ˆçš„æ£€ç´¢å’Œå¤„ç†ã€‚
"""

import json
import logging
import re
import requests
import sys
import os
import time
from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional, Tuple
import concurrent.futures

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥å¯¼å…¥ç›¸å…³æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
# ç§»é™¤SimpleRAGClientå¯¼å…¥
from clients.external_api_client import get_external_api_client
from clients.web_search_client import get_web_search_client
from config.settings import get_concurrency_manager, SmartConcurrencyManager

# ==============================================================================
# 1. æ•°æ®ç»“æ„ä¸è¾…åŠ©ç±»
# ==============================================================================

class SectionInfo:
    def __init__(self, **kwargs):
        for key, value in kwargs.items():
            setattr(self, key, value)

@dataclass
class ReActState:
    iteration: int = 0
    attempted_queries: List[str] = field(default_factory=list)
    retrieved_results: List[Dict] = field(default_factory=list)
    quality_scores: List[float] = field(default_factory=list)
    processed_pages: set = field(default_factory=set)  # è·Ÿè¸ªå·²å¤„ç†çš„é¡µæ•°

class ColoredLogger:
    COLORS = {
        'RESET': '\033[0m', 'BLUE': '\033[94m', 'GREEN': '\033[92m', 
        'YELLOW': '\033[93m', 'RED': '\033[91m', 'PURPLE': '\033[95m', 
        'CYAN': '\033[96m', 'WHITE': '\033[97m',
    }
    
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
    
    def _colorize(self, text: str, color: str) -> str:
        return f"{self.COLORS.get(color, '')}{text}{self.COLORS['RESET']}"
    
    def info(self, message: str): self.logger.info(message)
    def error(self, message: str): self.logger.error(message)
    def warning(self, message: str): self.logger.warning(message)
    def debug(self, message: str): self.logger.debug(message)
    def thought(self, content: str): self.logger.info(self._colorize(f"ğŸ’­ Thought: {content}", 'BLUE'))
    def input_tool(self, content: str): self.logger.info(self._colorize(f"ğŸ”§ Input: {content}", 'GREEN'))
    def observation(self, content: str): self.logger.info(self._colorize(f"ğŸ‘ï¸ Observation: {content}", 'YELLOW'))
    def reflection(self, content: str): self.logger.info(self._colorize(f"ğŸ¤” Reflection: {content}", 'CYAN'))
    def section_start(self, title: str): self.logger.info(self._colorize(f"\nğŸ“ å¼€å§‹å¤„ç†ç« èŠ‚: {title}", 'PURPLE'))
    def section_complete(self, title: str, iterations: int, quality: float): self.logger.info(self._colorize(f"âœ… ç« èŠ‚'{title}'å®Œæˆ | è¿­ä»£{iterations}æ¬¡ | æœ€ç»ˆè´¨é‡: {quality:.2f}", 'WHITE'))
    def iteration(self, current: int, total: int): self.logger.info(self._colorize(f"ğŸ”„ [Iteration {current}/{total}]", 'CYAN'))

# ==============================================================================
# 2. æ ¸å¿ƒAgentç±»
# ==============================================================================

class EnhancedReactAgent:
    def __init__(self, client: Any, concurrency_manager: SmartConcurrencyManager = None):
        self.client = client
        self.colored_logger = ColoredLogger(__name__)
        self.max_iterations = 3
        self.quality_threshold = 0.7
        
        # ä½¿ç”¨å¤–éƒ¨APIè¿›è¡Œæ–‡æ¡£æ£€ç´¢
        
        # å¤–éƒ¨APIå®¢æˆ·ç«¯
        self.external_api = get_external_api_client()
        
        # Webæœç´¢å®¢æˆ·ç«¯
        self.web_search_client = get_web_search_client()
        
        # æ™ºèƒ½å¹¶å‘ç®¡ç†å™¨
        self.concurrency_manager = concurrency_manager or get_concurrency_manager()
        self.max_workers = self.concurrency_manager.get_max_workers('react_agent')
        
        # æ™ºèƒ½é€Ÿç‡æ§åˆ¶å™¨
        self.rate_limiter = self.concurrency_manager.get_rate_limiter('react_agent')
        self.has_smart_control = self.concurrency_manager.has_smart_rate_control('react_agent')
        
        # æ€§èƒ½ç»Ÿè®¡
        self.react_stats = {
            'total_sections_processed': 0,
            'total_external_queries': 0,
            'successful_queries': 0,
            'failed_queries': 0,
            'total_processing_time': 0.0,
            'avg_quality_score': 0.0
        }
        
        self.query_strategies = {
            'direct': "ç›´æ¥ä½¿ç”¨æ ¸å¿ƒå…³é”®è¯æœç´¢", 
            'contextual': "ç»“åˆå†™ä½œæŒ‡å¯¼ä¸Šä¸‹æ–‡çš„è¯¦ç»†æŸ¥è¯¢", 
            'semantic': "æœç´¢ä¸ä¸»é¢˜ç›¸å…³çš„è¯­ä¹‰æ¦‚å¿µ", 
            'specific': "æœç´¢å…·ä½“çš„æ¡ˆä¾‹ã€æ•°æ®æˆ–æŠ€æœ¯æ ‡å‡†",
            'alternative': "ä½¿ç”¨åŒä¹‰è¯å’Œç›¸å…³æ¦‚å¿µè¿›è¡Œå‘æ•£æœç´¢"
        }
        
        status_msg = f"æ™ºèƒ½é€Ÿç‡æ§åˆ¶: {'å·²å¯ç”¨' if self.has_smart_control else 'ä¼ ç»Ÿæ¨¡å¼'}"
        self.colored_logger.info(f"EnhancedReactAgent åˆå§‹åŒ–å®Œæˆï¼Œå¹¶å‘çº¿ç¨‹æ•°: {self.max_workers}, {status_msg}")
        
        # æ£€æŸ¥å¤–éƒ¨APIæœåŠ¡çŠ¶æ€
        try:
            api_status = self.external_api.check_service_status()
            if api_status.get('status') == 'running':
                self.colored_logger.info(f"âœ… å¤–éƒ¨APIæœåŠ¡è¿æ¥æ­£å¸¸: {api_status.get('service', '')} v{api_status.get('version', '')}")
            else:
                self.colored_logger.warning(f"âš ï¸ å¤–éƒ¨APIæœåŠ¡çŠ¶æ€å¼‚å¸¸: {api_status}ï¼Œå°†ä½¿ç”¨æœ¬åœ°RAGä½œä¸ºå¤‡ç”¨")
        except Exception as e:
            self.colored_logger.error(f"âŒ å¤–éƒ¨APIæœåŠ¡è¿æ¥æ£€æŸ¥å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æœ¬åœ°RAGä½œä¸ºå¤‡ç”¨")
        
        # æ£€æŸ¥Webæœç´¢æœåŠ¡çŠ¶æ€ï¼ˆæ”¯æŒè·³è¿‡ï¼‰
        try:
            web_status = self.web_search_client.check_service_status()
            if web_status.get('status') == 'running':
                if web_status.get('skipped'):
                    self.colored_logger.info("âœ… Webæœç´¢æœåŠ¡å‡å®šå¯ç”¨ï¼ˆå·²è·³è¿‡å¥åº·æ£€æŸ¥ï¼‰")
                else:
                    self.colored_logger.info(f"âœ… Webæœç´¢æœåŠ¡è¿æ¥æ­£å¸¸: {web_status.get('service', '')}")
            else:
                self.colored_logger.warning(f"âš ï¸ Webæœç´¢æœåŠ¡çŠ¶æ€å¼‚å¸¸: {web_status}")
        except Exception as e:
            self.colored_logger.error(f"âŒ Webæœç´¢æœåŠ¡è¿æ¥æ£€æŸ¥å¤±è´¥: {e}")

    def set_max_workers(self, max_workers: int):
        """åŠ¨æ€è®¾ç½®æœ€å¤§çº¿ç¨‹æ•°"""
        self.max_workers = max_workers
        self.concurrency_manager.set_max_workers('react_agent', max_workers)
        self.colored_logger.info(f"ReactAgent çº¿ç¨‹æ•°å·²æ›´æ–°ä¸º: {max_workers}")

    def get_max_workers(self) -> int:
        """è·å–å½“å‰æœ€å¤§çº¿ç¨‹æ•°"""
        return self.max_workers

    def process_report_guide(self, report_guide_data: Dict[str, Any], project_name: str = "åŒ»çµå¤åº™") -> Dict[str, Any]:
        """å¤„ç†å®Œæ•´çš„æŠ¥å‘ŠæŒ‡å— - ä¸»å…¥å£ (å¹¶è¡Œå¤„ç†é¡¶å±‚ï¼Œé€’å½’å¤„ç†æ‰€æœ‰å±‚çº§)"""
        self.colored_logger.logger.info(f"ğŸ¤– ReActå¼€å§‹å¹¶è¡Œå¤„ç†æŠ¥å‘ŠæŒ‡å—... (é¡¹ç›®: {project_name}, çº¿ç¨‹æ•°: {self.max_workers})")
        result_data = json.loads(json.dumps(report_guide_data))
        self.current_project_name = project_name  # å­˜å‚¨é¡¹ç›®åç§°ä¾›åç»­ä½¿ç”¨

        # å¹¶è¡Œä»…ç”¨äºé¡¶å±‚sectionsï¼Œå­å±‚çº§åœ¨å„è‡ªä»»åŠ¡ä¸­é€’å½’ä¸²è¡Œå¤„ç†ï¼Œé™ä½ä»»åŠ¡è°ƒåº¦å¼€é”€
        tasks = []
        for part in result_data.get('report_guide', []):
            part_context = {'title': part.get('title', ''), 'goal': part.get('goal', '')}
            for section in part.get('sections', []):
                tasks.append((section, part_context, [part_context.get('title', '')]))

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_payload = {
                executor.submit(self._process_node_recursive, section, part_context, breadcrumb): (section, part_context)
                for section, part_context, breadcrumb in tasks
            }
            for future in concurrent.futures.as_completed(future_to_payload):
                try:
                    future.result()
                except Exception as exc:
                    section, _ = future_to_payload[future]
                    error_message = f"ç« èŠ‚ '{section.get('subtitle')}' åœ¨å¹¶è¡Œå¤„ç†ä¸­å‘ç”Ÿé”™è¯¯: {exc}"
                    self.colored_logger.error(error_message)
                    section['retrieved_data'] = error_message

        self.colored_logger.logger.info("\nâœ… æ‰€æœ‰ç« èŠ‚å¹¶è¡Œå¤„ç†å®Œæˆï¼")
        return result_data

    def _process_node_recursive(self, node: Dict[str, Any], part_context: Dict[str, str], breadcrumb: List[str]) -> None:
        """å¯¹å•ä¸ªèŠ‚ç‚¹æ‰§è¡ŒReActï¼Œå¹¶é€’å½’å¤„ç†å…¶å­èŠ‚ç‚¹ã€‚"""
        result = self._process_section_with_react(node, part_context)
        if isinstance(result, dict) and all(k in result for k in ['retrieved_text', 'retrieved_image', 'retrieved_table']):
            node['retrieved_text'] = result['retrieved_text']
            node['retrieved_image'] = result['retrieved_image']
            node['retrieved_table'] = result['retrieved_table']
            # æ·»åŠ Webæœç´¢ç»“æœå¤„ç†
            if 'retrieved_web' in result:
                node['retrieved_web'] = result['retrieved_web']
        else:
            node['retrieved_data'] = result

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        for child in node.get('subsections', []) or []:
            self._process_node_recursive(child, part_context, breadcrumb + [node.get('subtitle', '')])

    def _process_section_with_react(self, section_data: dict, part_context: dict) -> str:
        """ä¸ºå•ä¸ªç« èŠ‚å¯åŠ¨å¹¶ç®¡ç†ReActå¤„ç†æµç¨‹ã€‚"""
        subtitle = section_data.get('subtitle', '')
        self.colored_logger.section_start(subtitle)
        state = ReActState()
        section_context = {
            'subtitle': subtitle, 'how_to_write': section_data.get('how_to_write', ''),
            'part_title': part_context.get('title', ''), 'part_goal': part_context.get('goal', '')
        }
        retrieved_content = self._react_loop_for_section(section_context, state)
        self.colored_logger.section_complete(subtitle, state.iteration, max(state.quality_scores) if state.quality_scores else 0)
        return retrieved_content

    def _react_loop_for_section(self, section_context: Dict[str, str], state: ReActState) -> str:
        """ReActçš„æ ¸å¿ƒå¾ªç¯ - å¤šç»´åº¦å¹¶è¡ŒæŸ¥è¯¢æ¨¡å¼"""
        state.iteration = 1
        self.colored_logger.iteration(state.iteration, 1)
        
        # ç”Ÿæˆå¤šç»´åº¦æŸ¥è¯¢è®¡åˆ’
        multi_queries = self._generate_multi_dimensional_queries(section_context, state)
        if not multi_queries:
            self.colored_logger.thought("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å¤šç»´åº¦æŸ¥è¯¢è®¡åˆ’ï¼Œæå‰ç»“æŸã€‚")
            return self._synthesize_retrieved_results(section_context, state)

        self.colored_logger.thought(f"ç”Ÿæˆäº† {len(multi_queries)} ä¸ªç»´åº¦çš„æŸ¥è¯¢è®¡åˆ’")
        
        # å¹¶è¡Œæ‰§è¡Œå¤šä¸ªRAGæŸ¥è¯¢
        all_results = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(multi_queries), 3)) as executor:
            future_to_query = {
                executor.submit(self._execute_single_query, query_info, section_context, state): query_info
                for query_info in multi_queries
            }
            
            for future in concurrent.futures.as_completed(future_to_query):
                query_info = future_to_query[future]
                try:
                    results = future.result()
                    all_results.extend(results)
                    self.colored_logger.observation(f"ç»´åº¦'{query_info['dimension']}'æ£€ç´¢å®Œæˆ: {len(results)}æ¡ç»“æœ")
                except Exception as exc:
                    self.colored_logger.error(f"ç»´åº¦'{query_info['dimension']}'æŸ¥è¯¢å¤±è´¥: {exc}")
        
        # åˆå¹¶æ‰€æœ‰RAGç»“æœ
        state.retrieved_results.extend(all_results)
        
        self.colored_logger.reflection(f"RAGå¤šç»´åº¦æŸ¥è¯¢å®Œæˆ: æ€»è®¡{len(all_results)}æ¡ç»“æœ")
        
        # å¿…å®šæ‰§è¡ŒWebæœç´¢è¡¥å……ï¼šåˆ†æRAGç»“æœç¼ºå£åè¿›è¡Œé’ˆå¯¹æ€§Webæœç´¢
        self.colored_logger.thought(f"ğŸ¤” åˆ†æRAGæ£€ç´¢ç»“æœï¼Œè¯†åˆ«ä¿¡æ¯ç¼ºå£...")
        web_results = self._perform_intelligent_web_search(section_context, all_results)
        if web_results:
            all_results.extend(web_results)
            state.retrieved_results.extend(web_results)
            self.colored_logger.observation(f"ğŸŒ æ™ºèƒ½Webæœç´¢è¡¥å……: æ–°å¢ {len(web_results)} æ¡ç»“æœ")
        else:
            self.colored_logger.warning("ğŸŒ Webæœç´¢æœªè¿”å›ç»“æœ")
                
        return self._synthesize_retrieved_results(section_context, state)

    def _generate_multi_dimensional_queries(self, section_context: Dict[str, str], state: ReActState) -> List[Dict[str, str]]:
        """ç”Ÿæˆå¤šç»´åº¦æŸ¥è¯¢è®¡åˆ’"""
        # è·å–é¡¹ç›®åç§°ï¼Œç”¨äºç”Ÿæˆæ›´ç²¾å‡†çš„æŸ¥è¯¢
        project_name = getattr(self, 'current_project_name', '')
        
        prompt = f"""
ä½ æ˜¯ä¸“ä¸šçš„æŠ¥å‘Šç¼–åˆ¶ä¸“å®¶ï¼Œéœ€è¦ä¸ºç‰¹å®šé¡¹ç›®çš„æŠ¥å‘Šç« èŠ‚åˆ¶å®šç²¾å‡†çš„èµ„æ–™æ£€ç´¢è®¡åˆ’ã€‚

ã€é¡¹ç›®ä¿¡æ¯ã€‘: {project_name}
ã€ç›®æ ‡ç« èŠ‚ã€‘: {section_context['subtitle']}
ã€å†™ä½œè¦æ±‚ã€‘: {section_context['how_to_write']}

ã€æ ¸å¿ƒä»»åŠ¡ã€‘: æ·±åº¦åˆ†æå†™ä½œè¦æ±‚ï¼Œè¯†åˆ«å®Œæˆè¯¥ç« èŠ‚å†™ä½œçš„å¿…å¤‡èµ„æ–™ç±»å‹ï¼Œç”Ÿæˆç²¾å‡†çš„æ£€ç´¢æŸ¥è¯¢ã€‚

ã€åˆ†ææ­¥éª¤ã€‘:
1. ä»å†™ä½œè¦æ±‚ä¸­æå–å…³é”®ä¿¡æ¯è¦ç´ ï¼ˆæ•°æ®ã€æ”¿ç­–ã€æ ‡å‡†ã€æ¡ˆä¾‹ç­‰ï¼‰
2. ç»“åˆé¡¹ç›®ç‰¹ç‚¹ç¡®å®šæ£€ç´¢çš„ä¸šåŠ¡é¢†åŸŸå’ŒèŒƒå›´
3. é’ˆå¯¹æ¯ç±»å¿…å¤‡èµ„æ–™è®¾è®¡æœ€æœ‰æ•ˆçš„æ£€ç´¢è¯ç»„

ã€æŸ¥è¯¢ç”ŸæˆåŸåˆ™ã€‘:
1. ã€ç´§æ‰£å†™ä½œè¦æ±‚ã€‘: æŸ¥è¯¢å¿…é¡»ç›´æ¥æœåŠ¡äºå†™ä½œè¦æ±‚ä¸­çš„å…·ä½“å†…å®¹
2. ã€é¡¹ç›®ç‰¹å®šæ€§ã€‘: ç»“åˆé¡¹ç›®åç§°ä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆè¡Œä¸šã€åœ°åŸŸã€ç±»å‹ï¼‰
3. ã€èµ„æ–™å¯¼å‘ã€‘: é‡ç‚¹æ£€ç´¢èƒ½ç›´æ¥ç”¨äºå†™ä½œçš„å…·ä½“èµ„æ–™
4. ã€ç²¾å‡†ç®€æ´ã€‘: æ¯ä¸ªæŸ¥è¯¢2-4ä¸ªæ ¸å¿ƒå…³é”®è¯ï¼Œé¿å…å®½æ³›æ¦‚å¿µ

ã€è¾“å‡ºè¦æ±‚ã€‘: ä¸¥æ ¼è¿”å›JSONæ•°ç»„ï¼ŒåŒ…å«2-3ä¸ªæœ€å…³é”®çš„æ£€ç´¢ç»´åº¦:
[
  {{"dimension": "èµ„æ–™ç±»å‹æè¿°", "query": "ç²¾å‡†æŸ¥è¯¢è¯ç»„", "priority": "high/medium/low"}},
  {{"dimension": "èµ„æ–™ç±»å‹æè¿°", "query": "ç²¾å‡†æŸ¥è¯¢è¯ç»„", "priority": "high/medium/low"}}
]

ã€ç¤ºä¾‹å‚è€ƒã€‘:
- æ”¿ç­–ç±»èµ„æ–™: "èŒä¸šæ•™è‚²æ³• å®æ–½ç»†åˆ™" 
- æ ‡å‡†ç±»èµ„æ–™: "ä¸­èŒå­¦æ ¡ å»ºè®¾æ ‡å‡†"
- æ•°æ®ç±»èµ„æ–™: "æ¸…è¿œå¸‚ æ•™è‚²ç»Ÿè®¡"
- æ¡ˆä¾‹ç±»èµ„æ–™: "èŒä¸šæ•™è‚²åŸºåœ° å»ºè®¾æ¡ˆä¾‹"
"""
        
        try:
            response_str = self.client.generate(prompt)
            # æå–JSONæ•°ç»„
            import re
            json_match = re.search(r'\[.*?\]', response_str, re.DOTALL)
            if json_match:
                queries = json.loads(json_match.group(0))
                # éªŒè¯æ ¼å¼
                valid_queries = []
                for q in queries:
                    if isinstance(q, dict) and all(k in q for k in ['dimension', 'query', 'priority']):
                        valid_queries.append(q)
                
                self.colored_logger.debug(f"ğŸ¯ ç”Ÿæˆå¤šç»´åº¦æŸ¥è¯¢: {[q['dimension'] for q in valid_queries]}")
                return valid_queries
            else:
                self.colored_logger.error("æœªèƒ½ä»LLMå“åº”ä¸­æå–æœ‰æ•ˆçš„JSONæ•°ç»„")
                return []
        except Exception as e:
            self.colored_logger.error(f"ç”Ÿæˆå¤šç»´åº¦æŸ¥è¯¢å¤±è´¥: {e}")
            return []

    def _execute_single_query(self, query_info: Dict[str, str], section_context: Dict[str, str], state: ReActState) -> List[Dict]:
        """æ‰§è¡Œå•ä¸ªç»´åº¦çš„æŸ¥è¯¢"""
        query = query_info['query']
        dimension = query_info['dimension']
        
        self.colored_logger.input_tool(f"ğŸ” {dimension} | Query: {query}")
        
        # è®°å½•æŸ¥è¯¢å°è¯•
        state.attempted_queries.append(f"{dimension}:{query}")
        
        # æ‰§è¡ŒæŸ¥è¯¢
        results = self._observe_section_results(query, section_context, state)
        
        # ä¸ºç»“æœæ·»åŠ ç»´åº¦æ ‡è®°
        for result in results:
            result['dimension'] = dimension
            result['priority'] = query_info.get('priority', 'medium')
        
        return results

    def _perform_intelligent_web_search(self, section_context: Dict[str, str], rag_results: List[Dict]) -> List[Dict[str, Any]]:
        """åŸºäºRAGç»“æœåˆ†æè¿›è¡Œæ™ºèƒ½Webæœç´¢"""
        try:
            # åˆ†æRAGç»“æœçš„å†…å®¹ç¼ºå£
            web_query = self._analyze_rag_gaps_and_generate_query(section_context, rag_results)
            if not web_query:
                self.colored_logger.warning("âŒ æœªèƒ½ç”ŸæˆWebæœç´¢æŸ¥è¯¢ï¼Œè·³è¿‡Webæœç´¢è¡¥å……")
                return []
            
            self.colored_logger.input_tool(f"ğŸŒ æ™ºèƒ½Webæœç´¢ | Query: {web_query}")
            
            # æ‰§è¡ŒWebæœç´¢
            search_results = self.web_search_client.search(
                query=web_query,
                engines=["serp"],
                max_results=5
            )
            
            if not search_results:
                self.colored_logger.warning("ğŸŒ Webæœç´¢æœªè¿”å›ç»“æœ")
                return []
            
            # æ ¼å¼åŒ–Webæœç´¢ç»“æœ
            formatted_results = self.web_search_client.format_search_results(search_results)
            
            # ä¸ºWebæœç´¢ç»“æœæ·»åŠ æ ‡è®°
            for result in formatted_results:
                result['dimension'] = 'web_intelligent'
                result['priority'] = 'medium'  # Webæœç´¢ä½œä¸ºè¡¥å……ï¼Œä¼˜å…ˆçº§ä¸­ç­‰
                result['type'] = 'web_text'
            
            return formatted_results
            
        except Exception as e:
            self.colored_logger.error(f"âŒ æ™ºèƒ½Webæœç´¢å¤±è´¥: {e}")
            return []

    def _analyze_rag_gaps_and_generate_query(self, section_context: Dict[str, str], rag_results: List[Dict]) -> Optional[str]:
        """åˆ†æRAGç»“æœç¼ºå£å¹¶ç”ŸæˆWebæœç´¢æŸ¥è¯¢"""
        
        # å®‰å…¨åœ°å¤„ç†RAGç»“æœå†…å®¹
        def safe_content_summary(results):
            if not results:
                return "æ— æ£€ç´¢ç»“æœ"
            
            content_snippets = []
            for result in results[:3]:  # åªåˆ†æå‰3ä¸ªç»“æœ
                content = result.get('content', '')
                if isinstance(content, str) and content.strip():
                    content_snippets.append(content[:80])  # å–å‰80å­—ç¬¦
            
            return " | ".join(content_snippets) if content_snippets else "æ£€ç´¢ç»“æœä¸ºç©º"
        
        rag_summary = safe_content_summary(rag_results)
        
        # è·å–é¡¹ç›®åç§°ï¼Œç”¨äºç”Ÿæˆæ›´ç²¾å‡†çš„æŸ¥è¯¢
        project_name = getattr(self, 'current_project_name', '')
        
        prompt = f"""
ä½ æ˜¯ä¸“ä¸šçš„æŠ¥å‘Šç¼–åˆ¶ä¸“å®¶ï¼Œéœ€è¦ä¸ºå½“å‰æŠ¥å‘Šç« èŠ‚ç”Ÿæˆç²¾å‡†çš„Webæœç´¢æŸ¥è¯¢ã€‚

ã€é¡¹ç›®åç§°ã€‘: {project_name}
ã€ç›®æ ‡ç« èŠ‚ã€‘: {section_context['subtitle']}
ã€å†™ä½œè¦æ±‚ã€‘: {section_context['how_to_write']}
ã€RAGå·²æœ‰å†…å®¹ã€‘: {rag_summary}

ã€æ ¸å¿ƒä»»åŠ¡ã€‘: åŸºäºRAGæ£€ç´¢ç»“æœçš„ä¸è¶³ï¼Œç”Ÿæˆ1ä¸ªç²¾å‡†çš„Webæœç´¢æŸ¥è¯¢æ¥è¡¥å……å…³é”®ä¿¡æ¯

ã€æŸ¥è¯¢ç”ŸæˆåŸåˆ™ã€‘:
1. ã€ä¸»é¢˜èšç„¦ã€‘: ç´§æ‰£é¡¹ç›®åç§°å’Œç« èŠ‚ä¸»é¢˜ï¼Œæå–æ ¸å¿ƒä¸šåŠ¡é¢†åŸŸå…³é”®è¯
2. ã€å†…å®¹äº’è¡¥ã€‘: é‡ç‚¹è¡¥å……RAGç¼ºå¤±çš„ä¿¡æ¯ï¼ˆæ”¿ç­–æ³•è§„ã€æ ‡å‡†è§„èŒƒã€æ¡ˆä¾‹å‚è€ƒã€æœ€æ–°æ•°æ®ï¼‰
3. ã€ç²¾å‡†ç®€æ´ã€‘: æŸ¥è¯¢è¯æ§åˆ¶åœ¨3-6ä¸ªæ ¸å¿ƒè¯æ±‡ï¼Œé¿å…å†—é•¿æ‹¼æ¥
4. ã€æ—¶æ•ˆä¼˜å…ˆã€‘: ä¼˜å…ˆè·å–æœ€æ–°çš„è¡Œä¸šä¿¡æ¯å’Œæ”¿ç­–åŠ¨æ€

ã€è¾“å‡ºè¦æ±‚ã€‘: 
- åªè¿”å›æœç´¢æŸ¥è¯¢è¯ï¼Œç”¨ç©ºæ ¼åˆ†éš”
- é•¿åº¦é™åˆ¶ï¼š3-6ä¸ªå…³é”®è¯
- å¿…é¡»è´´åˆé¡¹ç›®ä¸»é¢˜å’Œç« èŠ‚å†…å®¹
- ä¸è¦ä»»ä½•è§£é‡Šæˆ–å…¶ä»–å†…å®¹

ã€æŸ¥è¯¢æ„å»ºé€»è¾‘ã€‘:
1. ä»é¡¹ç›®åç§°ä¸­æå–è¡Œä¸š/é¢†åŸŸå…³é”®è¯
2. ç»“åˆç« èŠ‚è¦æ±‚ç¡®å®šä¿¡æ¯ç±»å‹ï¼ˆæ”¿ç­–/æ ‡å‡†/æ•°æ®/æ¡ˆä¾‹ï¼‰
3. ç”Ÿæˆç®€æ´æœ‰æ•ˆçš„æœç´¢è¯ç»„åˆ
"""
        
        try:
            response = self.client.generate(prompt)
            # æå–å¹¶æ¸…ç†æŸ¥è¯¢è¯
            web_query = response.strip().replace('\n', ' ').replace('\r', ' ')
            web_query = ' '.join(web_query.split())  # ç§»é™¤å¤šä½™ç©ºæ ¼
            
            # ç§»é™¤å¯èƒ½çš„å¼•å·å’Œå…¶ä»–æ ‡ç‚¹ç¬¦å·
            web_query = web_query.replace('"', '').replace("'", '').replace('ï¼Œ', ' ').replace('ã€', ' ')
            web_query = ' '.join(web_query.split())  # å†æ¬¡æ¸…ç†ç©ºæ ¼
            
            # é™åˆ¶æŸ¥è¯¢è¯æ•°é‡ï¼ˆ3-6ä¸ªè¯ï¼‰
            query_words = web_query.split()
            if len(query_words) > 6:
                web_query = ' '.join(query_words[:6])
            
            # ç¡®ä¿æŸ¥è¯¢é•¿åº¦åˆç†
            if len(web_query) > 50:
                web_query = web_query[:50].rsplit(' ', 1)[0]
            
            if web_query and len(web_query.split()) >= 2:
                self.colored_logger.debug(f"ğŸ¯ æ™ºèƒ½ç”ŸæˆWebæŸ¥è¯¢: {web_query}")
                return web_query
            else:
                self.colored_logger.warning(f"LLMç”Ÿæˆçš„æŸ¥è¯¢ä¸ç¬¦åˆè¦æ±‚: '{response}' -> '{web_query}'ï¼Œè·³è¿‡Webæœç´¢")
                return None
                
        except Exception as e:
            self.colored_logger.error(f"åˆ†æRAGç¼ºå£å¤±è´¥: {e}ï¼Œè·³è¿‡Webæœç´¢")
            return None

    def _perform_web_search_supplement(self, section_context: Dict[str, str], multi_queries: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒWebæœç´¢è¡¥å……"""
        try:
            # ç”ŸæˆWebæœç´¢æŸ¥è¯¢
            web_query = self._generate_web_search_query(section_context, multi_queries)
            if not web_query:
                return []
            
            self.colored_logger.input_tool(f"ğŸŒ Webæœç´¢è¡¥å…… | Query: {web_query}")
            
            # æ‰§è¡ŒWebæœç´¢
            search_results = self.web_search_client.search(
                query=web_query,
                engines=["serp"],
                max_results=5  # é™åˆ¶Webæœç´¢ç»“æœæ•°é‡
            )
            
            if not search_results:
                self.colored_logger.warning("ğŸŒ Webæœç´¢æœªè¿”å›ç»“æœ")
                return []
            
            # æ ¼å¼åŒ–Webæœç´¢ç»“æœ
            formatted_results = self.web_search_client.format_search_results(search_results)
            
            # ä¸ºWebæœç´¢ç»“æœæ·»åŠ ç»´åº¦æ ‡è®°
            for result in formatted_results:
                result['dimension'] = 'web_supplement'
                result['priority'] = 'medium'  # Webæœç´¢ç»“æœä½œä¸ºè¡¥å……ï¼Œä¼˜å…ˆçº§ä¸­ç­‰
            
            return formatted_results
            
        except Exception as e:
            self.colored_logger.error(f"âŒ Webæœç´¢è¡¥å……å¤±è´¥: {e}")
            return []
    
    def _generate_web_search_query(self, section_context: Dict[str, str], multi_queries: List[Dict[str, str]]) -> Optional[str]:
        """ç”ŸæˆWebæœç´¢æŸ¥è¯¢è¯"""
        try:
            # æå–ç« èŠ‚æ ‡é¢˜çš„å…³é”®ä¿¡æ¯
            subtitle = section_context.get('subtitle', '')
            
            # æ„å»ºWebæœç´¢æŸ¥è¯¢
            # ä¼˜å…ˆä½¿ç”¨æœ€é‡è¦çš„ç»´åº¦æŸ¥è¯¢
            primary_queries = [q['query'] for q in multi_queries if q.get('priority') == 'high']
            if not primary_queries:
                primary_queries = [q['query'] for q in multi_queries[:1]]  # å–ç¬¬ä¸€ä¸ªæŸ¥è¯¢
            
            if primary_queries:
                # ç»“åˆç« èŠ‚æ ‡é¢˜å’Œä¸»è¦æŸ¥è¯¢æ„å»ºWebæœç´¢è¯
                web_query = f"{primary_queries[0]} {subtitle}".strip()
                # æ¸…ç†æŸ¥è¯¢è¯ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
                web_query = ' '.join(web_query.split())
                return web_query[:100]  # é™åˆ¶æŸ¥è¯¢é•¿åº¦
            
            return None
            
        except Exception as e:
            self.colored_logger.error(f"âŒ ç”ŸæˆWebæœç´¢æŸ¥è¯¢å¤±è´¥: {e}")
            return None

    def _deduplicate_results(self, results: List[Dict], result_type: str) -> List[Dict]:
        """æ™ºèƒ½å»é‡å¤„ç†"""
        if not results:
            return results
        
        # æ ¹æ®ä¸åŒç±»å‹é‡‡ç”¨ä¸åŒçš„å»é‡ç­–ç•¥
        if result_type == 'text':
            return self._deduplicate_text_results(results)
        elif result_type == 'image':
            return self._deduplicate_image_results(results)
        elif result_type == 'table':
            return self._deduplicate_table_results(results)
        elif result_type == 'web_text':
            return self._deduplicate_web_results(results)
        else:
            return results
    
    def _deduplicate_text_results(self, results: List[Dict]) -> List[Dict]:
        """æ–‡æœ¬ç»“æœå»é‡ï¼šåŸºäºå†…å®¹ç›¸ä¼¼åº¦å’Œé¡µç """
        if len(results) <= 1:
            return results
        
        deduplicated = []
        seen_pages = set()
        seen_content_hashes = set()
        
        # æŒ‰è´¨é‡åˆ†æ•°æ’åºï¼Œä¼˜å…ˆä¿ç•™é«˜è´¨é‡ç»“æœ
        sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
        
        for result in sorted_results:
            page_num = result.get('page_number', '')
            content = result.get('content', '')
            
            # ç”Ÿæˆå†…å®¹hashç”¨äºå»é‡
            content_hash = hash(content[:200])  # ä½¿ç”¨å‰200å­—ç¬¦ç”Ÿæˆhash
            
            # å»é‡é€»è¾‘ï¼š
            # 1. ç›¸åŒé¡µç çš„å†…å®¹åªä¿ç•™ä¸€ä¸ªï¼ˆè´¨é‡æœ€é«˜çš„ï¼‰
            # 2. å†…å®¹é«˜åº¦ç›¸ä¼¼çš„åªä¿ç•™ä¸€ä¸ª
            if page_num not in seen_pages and content_hash not in seen_content_hashes:
                deduplicated.append(result)
                if page_num:
                    seen_pages.add(page_num)
                seen_content_hashes.add(content_hash)
                
                # é™åˆ¶æ¯ç§ç±»å‹çš„æœ€å¤§ç»“æœæ•°
                if len(deduplicated) >= 8:  # æ–‡æœ¬ç»“æœæœ€å¤šä¿ç•™8æ¡
                    break
        
        self.colored_logger.debug(f"ğŸ“ æ–‡æœ¬å»é‡: {len(results)} -> {len(deduplicated)}")
        return deduplicated
    
    def _deduplicate_image_results(self, results: List[Dict]) -> List[Dict]:
        """å›¾ç‰‡ç»“æœå»é‡ï¼šåŸºäºè·¯å¾„å’Œé¡µç """
        if len(results) <= 1:
            return results
        
        deduplicated = []
        seen_paths = set()
        seen_pages = set()
        
        # æŒ‰è´¨é‡åˆ†æ•°æ’åº
        sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
        
        for result in sorted_results:
            path = result.get('path', '')
            page_num = result.get('page_number', '')
            
            # å›¾ç‰‡å»é‡ï¼šç›¸åŒè·¯å¾„æˆ–ç›¸åŒé¡µç çš„å›¾ç‰‡åªä¿ç•™ä¸€ä¸ª
            path_key = path.strip() if path else f"page_{page_num}"
            
            if path_key not in seen_paths:
                deduplicated.append(result)
                seen_paths.add(path_key)
                
                # é™åˆ¶å›¾ç‰‡ç»“æœæ•°é‡
                if len(deduplicated) >= 6:  # å›¾ç‰‡ç»“æœæœ€å¤šä¿ç•™6æ¡
                    break
        
        self.colored_logger.debug(f"ğŸ–¼ï¸ å›¾ç‰‡å»é‡: {len(results)} -> {len(deduplicated)}")
        return deduplicated
    
    def _deduplicate_table_results(self, results: List[Dict]) -> List[Dict]:
        """è¡¨æ ¼ç»“æœå»é‡ï¼šåŸºäºé¡µç å’Œå†…å®¹"""
        if len(results) <= 1:
            return results
        
        deduplicated = []
        seen_pages = set()
        
        # æŒ‰è´¨é‡åˆ†æ•°æ’åº
        sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
        
        for result in sorted_results:
            page_num = result.get('page_number', '')
            
            # è¡¨æ ¼å»é‡ï¼šç›¸åŒé¡µç çš„è¡¨æ ¼åªä¿ç•™ä¸€ä¸ª
            if page_num not in seen_pages:
                deduplicated.append(result)
                if page_num:
                    seen_pages.add(page_num)
                
                # é™åˆ¶è¡¨æ ¼ç»“æœæ•°é‡
                if len(deduplicated) >= 4:  # è¡¨æ ¼ç»“æœæœ€å¤šä¿ç•™4æ¡
                    break
        
        self.colored_logger.debug(f"ğŸ“‹ è¡¨æ ¼å»é‡: {len(results)} -> {len(deduplicated)}")
        return deduplicated
    
    def _deduplicate_web_results(self, results: List[Dict]) -> List[Dict]:
        """Webæœç´¢ç»“æœå»é‡ï¼šåŸºäºURLå’Œå†…å®¹ç›¸ä¼¼åº¦"""
        if len(results) <= 1:
            return results
        
        deduplicated = []
        seen_urls = set()
        seen_content_hashes = set()
        
        # æŒ‰è´¨é‡åˆ†æ•°æ’åº
        sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)
        
        for result in sorted_results:
            url = result.get('url', '')
            content = result.get('content', '')
            
            # ç”Ÿæˆå†…å®¹hashç”¨äºå»é‡
            content_hash = hash(content[:300])  # ä½¿ç”¨å‰300å­—ç¬¦ç”Ÿæˆhash
            
            # Webç»“æœå»é‡ï¼šç›¸åŒURLæˆ–ç›¸ä¼¼å†…å®¹åªä¿ç•™ä¸€ä¸ª
            url_key = url.strip() if url else f"content_{content_hash}"
            
            if url_key not in seen_urls and content_hash not in seen_content_hashes:
                deduplicated.append(result)
                if url:
                    seen_urls.add(url_key)
                seen_content_hashes.add(content_hash)
                
                # é™åˆ¶Webæœç´¢ç»“æœæ•°é‡
                if len(deduplicated) >= 3:  # Webæœç´¢ç»“æœæœ€å¤šä¿ç•™3æ¡
                    break
        
        self.colored_logger.debug(f"ğŸŒ Webç»“æœå»é‡: {len(results)} -> {len(deduplicated)}")
        return deduplicated

    def _reason_and_act_for_section(self, section_context: Dict[str, str], state: ReActState) -> Optional[Dict[str, str]]:
        """åˆå¹¶æ¨ç†å’Œè¡ŒåŠ¨é˜¶æ®µ"""
        used_strategies = {q.split(':')[0] for q in state.attempted_queries if ':' in q}
        available_strategies = {k: v for k, v in self.query_strategies.items() if k not in used_strategies} or self.query_strategies
        prompt = f"""
ä½œä¸ºä¸€åä¸“ä¸šçš„ä¿¡æ¯æ£€ç´¢åˆ†æå¸ˆï¼Œä¸ºæŠ¥å‘Šç« èŠ‚åˆ¶å®šæ£€ç´¢è®¡åˆ’ã€‚
ã€ç›®æ ‡ç« èŠ‚ã€‘: {section_context['subtitle']}
ã€å†™ä½œæŒ‡å¯¼ã€‘: {section_context['how_to_write']}
ã€å†å²å°è¯•ã€‘: å·²å°è¯•æŸ¥è¯¢: {state.attempted_queries[-3:]}, å†å²è´¨é‡: {state.quality_scores[-3:]}
ã€å¯ç”¨ç­–ç•¥ã€‘: {json.dumps(available_strategies, ensure_ascii=False)}
ã€ä»»åŠ¡ã€‘: 1.åˆ†æç°çŠ¶ã€‚2.é€‰æ‹©ä¸€ä¸ªæœ€ä½³ç­–ç•¥ã€‚3.ç”Ÿæˆ3-5ä¸ªå…³é”®è¯ã€‚
ã€è¾“å‡ºæ ¼å¼ã€‘: å¿…é¡»ä¸¥æ ¼è¿”å›ä»¥ä¸‹JSONæ ¼å¼:
{{
  "analysis": "ç®€è¦åˆ†æï¼ˆ100å­—å†…ï¼‰",
  "strategy": "é€‰æ‹©çš„ç­–ç•¥åç§°",
  "keywords": "ç”¨é€—å·åˆ†éš”çš„å…³é”®è¯"
}}"""
        try:
            response_str = self.client.generate(prompt)
            match = re.search(r'\{.*\}', response_str, re.DOTALL)
            action_plan = json.loads(match.group(0))
            if all(k in action_plan for k in ['analysis', 'strategy', 'keywords']):
                return action_plan
            self.colored_logger.error(f"LLMè¿”å›çš„JSONæ ¼å¼ä¸å®Œæ•´: {action_plan}")
            return None
        except Exception as e:
            self.colored_logger.error(f"æ¨ç†ä¸è¡ŒåŠ¨é˜¶æ®µå‡ºé”™: {e}")
            return None

    def _observe_section_results(self, query: str, section_context: Dict[str, str], state: ReActState = None) -> List[Dict]:
        """è§‚å¯Ÿé˜¶æ®µï¼ˆä½¿ç”¨å¤–éƒ¨APIè¿›è¡Œæ–‡æ¡£æœç´¢ï¼‰"""
        query_start_time = time.time()
        
        try:
            # æ™ºèƒ½é€Ÿç‡æ§åˆ¶
            if self.has_smart_control:
                delay = self.rate_limiter.get_delay()
                if delay > 0:
                    time.sleep(delay)
            
            # ä½¿ç”¨å¤–éƒ¨APIè¿›è¡Œæ–‡æ¡£æœç´¢
            all_results = []
            
            # è®°å½•å¤–éƒ¨APIæŸ¥è¯¢å¼€å§‹
            self.react_stats['total_external_queries'] += 1
            
            # æ‰§è¡Œå¤–éƒ¨APIæ–‡æ¡£æœç´¢
            api_start_time = time.time()
            
            # å¤šç»´åº¦æŸ¥è¯¢æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ç²¾å‡†æŸ¥è¯¢è¯ç»„
            combined_query = query.strip()
            
            # è®°å½•æŸ¥è¯¢ä¿¡æ¯
            self.colored_logger.debug(f"ğŸ” æ‰§è¡ŒæŸ¥è¯¢: '{combined_query}'")
            
            # ä½¿ç”¨æ··åˆå†…å®¹æœç´¢API
            search_results = self.external_api.document_search(
                query=combined_query,
                project_name=getattr(self, 'current_project_name', 'åŒ»çµå¤åº™')
            )
            
            api_response_time = time.time() - api_start_time
            
            if search_results:
                # å¤„ç†æ··åˆå†…å®¹æœç´¢APIè¿”å›ç»“æœ
                all_results = []
                
                # è·å–æœç´¢ç»“æœæ•°ç»„
                results_data = search_results.get('data', {}).get('results', [])
                self.colored_logger.debug(f"ğŸ” æ··åˆå†…å®¹æœç´¢ç»“æœæ•°é‡: {len(results_data)}")
                
                for item in results_data:
                    page_number = item.get('page_number', 'N/A')
                    content = item.get('content', '')
                    images = item.get('images', [])
                    similarity = item.get('similarity', 0.0)
                    rerank_score = item.get('rerank_score', 0.0)
                    mixed_score = item.get('mixed_score', 0.0)
                    source_type = item.get('source_type', 'unknown')
                    
                    self.colored_logger.debug(f"ğŸ“„ å¤„ç†ç¬¬{page_number}é¡µï¼Œç±»å‹: {source_type}ï¼Œæ··åˆåˆ†æ•°: {mixed_score:.3f}")
                    
                    # æ ¹æ®source_typeç¡®å®šå†…å®¹ç±»å‹
                    if source_type == 'page_text':
                        # æ–‡æœ¬å†…å®¹
                        all_results.append({
                            'content': f"{content}",
                            'source': f"ç¬¬{page_number}é¡µæ–‡æœ¬ (æ··åˆåˆ†æ•°: {mixed_score:.3f})",
                            'type': 'text',
                            'score': mixed_score,
                            'page_number': page_number,
                            'similarity': similarity,
                            'rerank_score': rerank_score
                        })
                        
                        # å¤„ç†è¯¥é¡µé¢åŒ…å«çš„å›¾ç‰‡
                        for image_url in images:
                            clean_url = image_url.strip().strip('`').strip()
                            if clean_url:
                                all_results.append({
                                    'content': f"[ç¬¬{page_number}é¡µ] å›¾ç‰‡",
                                    'source': f"ç¬¬{page_number}é¡µå›¾ç‰‡ (æ··åˆåˆ†æ•°: {mixed_score:.3f})",
                                    'type': 'image',
                                    'score': mixed_score,
                                    'page_number': page_number,
                                    'path': clean_url,
                                    'description': f"ç¬¬{page_number}é¡µå›¾ç‰‡"
                                })
                    
                    elif source_type == 'detailed_description':
                        # å›¾ç‰‡æè¿°å†…å®¹
                        for image_url in images:
                            clean_url = image_url.strip().strip('`').strip()
                            if clean_url:
                                all_results.append({
                                    'content': f"å›¾ç‰‡æè¿°: {content}",
                                    'source': f"ç¬¬{page_number}é¡µå›¾ç‰‡æè¿° (æ··åˆåˆ†æ•°: {mixed_score:.3f})",
                                    'type': 'image',
                                    'score': mixed_score,
                                    'page_number': page_number,
                                    'path': clean_url,
                                    'description': content,
                                    'detailed_description': content,
                                    'similarity': similarity,
                                    'rerank_score': rerank_score
                                })
                    
                    else:
                        # å…¶ä»–ç±»å‹å†…å®¹ï¼Œä½œä¸ºé€šç”¨å¤„ç†
                        all_results.append({
                            'content': f"{content}",
                            'source': f"ç¬¬{page_number}é¡µ{source_type} (æ··åˆåˆ†æ•°: {mixed_score:.3f})",
                            'type': 'text',
                            'score': mixed_score,
                            'page_number': page_number,
                            'similarity': similarity,
                            'rerank_score': rerank_score
                        })

                
                # åˆ†æ®µæœç´¢æ¨¡å¼ï¼Œä¸è¿›è¡Œé¡µæ•°å»é‡
                
                total_text = len([r for r in all_results if r.get('type') == 'text'])
                total_image = len([r for r in all_results if r.get('type') == 'image'])
                total_table = len([r for r in all_results if r.get('type') == 'table'])
                
                # æ˜¾ç¤ºæ£€ç´¢ç»“æœç»Ÿè®¡
                self.colored_logger.observation(f"âœ… æ··åˆå†…å®¹æœç´¢æˆåŠŸï¼Œè·å¾— {len(all_results)} æ¡ç»“æœ "
                                              f"(æ–‡æœ¬:{total_text}, å›¾ç‰‡:{total_image}, è¡¨æ ¼:{total_table})")
            else:
                self.colored_logger.observation("ğŸ“­ æ£€ç´¢æœªè¿”å›ç»“æœ")
                all_results = []
            
            # è®°å½•æˆåŠŸçš„æŸ¥è¯¢
            if self.has_smart_control:
                self.concurrency_manager.record_api_request(
                    agent_name='react_agent',
                    success=True,
                    response_time=api_response_time
                )
            self.react_stats['successful_queries'] += 1
            
            return all_results
            
        except Exception as e:
            # è®°å½•å¤±è´¥çš„æŸ¥è¯¢
            query_response_time = time.time() - query_start_time
            if self.has_smart_control:
                error_type = self._classify_react_error(str(e))
                self.concurrency_manager.record_api_request(
                    agent_name='react_agent',
                    success=False,
                    response_time=query_response_time,
                    error_type=error_type
                )
            self.react_stats['failed_queries'] += 1
            
            self.colored_logger.error(f"è§‚å¯Ÿé˜¶æ®µå¤±è´¥: {e}")
            return []
    


    def _classify_react_error(self, error_message: str) -> str:
        """æ™ºèƒ½é”™è¯¯åˆ†ç±» - ReAct Agentä¸“ç”¨"""
        error_msg = error_message.lower()
        
        if 'rate limit' in error_msg or '429' in error_msg:
            return 'rate_limit'
        elif 'timeout' in error_msg:
            return 'timeout'
        elif 'network' in error_msg or 'connection' in error_msg:
            return 'network'
        elif 'rag' in error_msg or 'retrieval' in error_msg:
            return 'client_error'  # RAGæ£€ç´¢é”™è¯¯è§†ä¸ºå®¢æˆ·ç«¯é”™è¯¯
        elif '5' in error_msg[:2]:  # 5xx errors
            return 'server_error'
        elif '4' in error_msg[:2]:  # 4xx errors
            return 'client_error'
        else:
            return 'unknown'

    def _evaluate_section_results_quality(self, results: List[Dict], section_context: Dict[str, str], query: str) -> float:
        """è¯„ä¼°ç»“æœè´¨é‡"""
        if not results: return 0.0
        
        # å®‰å…¨åœ°å¤„ç†å†…å®¹ï¼Œç¡®ä¿è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        def safe_content_str(result):
            content = result.get('content', result)
            if isinstance(content, (list, dict)):
                return str(content)[:150]
            return str(content)[:150]
        
        evaluation_prompt = f"""
è¯„ä¼°ä»¥ä¸‹æ£€ç´¢ç»“æœå¯¹ç« èŠ‚å†™ä½œçš„é€‚ç”¨æ€§ï¼š
ã€ç›®æ ‡ç« èŠ‚ã€‘: {section_context['subtitle']}
ã€å†™ä½œæŒ‡å¯¼ã€‘: {section_context['how_to_write']}
ã€æœ¬æ¬¡æŸ¥è¯¢ã€‘: {query}
ã€æ£€ç´¢ç»“æœã€‘: {chr(10).join(f"- {safe_content_str(r)}..." for r in results[:3])}
ã€è¦æ±‚ã€‘: ç»¼åˆè¯„ä¼°åï¼Œåªè¿”å›ä¸€ä¸ª0.0åˆ°1.0çš„å°æ•°è¯„åˆ†ã€‚"""
        try:
            response = self.client.generate(evaluation_prompt)
            score_match = re.search(r'0?\.\d+|[01]', response)
            return max(0.0, min(1.0, float(score_match.group()))) if score_match else 0.2
        except Exception: return 0.1

    def _evaluate_overall_rag_quality(self, all_results: List[Dict], section_context: Dict[str, str]) -> float:
        """å¯¹æ‰€æœ‰RAGç»“æœè¿›è¡Œæ•´ä½“è´¨é‡è¯„ä¼°"""
        if not all_results: 
            return 0.0
        
        # å®‰å…¨åœ°å¤„ç†å†…å®¹ï¼Œç¡®ä¿è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        def safe_content_str(result):
            content = result.get('content', result)
            if isinstance(content, (list, dict)):
                return str(content)[:150]
            return str(content)[:150]
        
        # ç»Ÿè®¡ä¸åŒç±»å‹çš„ç»“æœ
        text_count = len([r for r in all_results if r.get('type') == 'text'])
        image_count = len([r for r in all_results if r.get('type') == 'image'])
        table_count = len([r for r in all_results if r.get('type') == 'table'])
        
        evaluation_prompt = f"""
è¯„ä¼°ä»¥ä¸‹RAGæ£€ç´¢ç»“æœå¯¹ç« èŠ‚å†™ä½œçš„æ•´ä½“é€‚ç”¨æ€§ï¼š

ã€ç›®æ ‡ç« èŠ‚ã€‘: {section_context['subtitle']}
ã€å†™ä½œæŒ‡å¯¼ã€‘: {section_context['how_to_write']}
ã€æ£€ç´¢ç»“æœç»Ÿè®¡ã€‘: æ–‡æœ¬{text_count}æ¡, å›¾ç‰‡{image_count}æ¡, è¡¨æ ¼{table_count}æ¡, æ€»è®¡{len(all_results)}æ¡
ã€ç»“æœæ ·æœ¬ã€‘: {chr(10).join(f"- {safe_content_str(r)}..." for r in all_results[:5])}

ã€è¯„ä¼°è¦æ±‚ã€‘: 
1. ç»¼åˆè€ƒè™‘ç»“æœçš„æ•°é‡ã€è´¨é‡ã€ç›¸å…³æ€§å’Œå®Œæ•´æ€§
2. è¯„ä¼°æ˜¯å¦èƒ½æ”¯æ’‘è¯¥ç« èŠ‚çš„å†™ä½œéœ€æ±‚
3. åªè¿”å›ä¸€ä¸ª0.0åˆ°1.0çš„å°æ•°è¯„åˆ†ï¼Œä¸è¦å…¶ä»–å†…å®¹

è¯„åˆ†æ ‡å‡†ï¼š
- 0.8-1.0: ç»“æœä¸°å¯Œä¸”é«˜åº¦ç›¸å…³ï¼Œå®Œå…¨æ”¯æ’‘å†™ä½œ
- 0.6-0.8: ç»“æœè¾ƒå¥½ï¼ŒåŸºæœ¬æ”¯æ’‘å†™ä½œéœ€æ±‚
- 0.4-0.6: ç»“æœä¸€èˆ¬ï¼Œéƒ¨åˆ†æ”¯æ’‘å†™ä½œ
- 0.0-0.4: ç»“æœä¸è¶³æˆ–ç›¸å…³æ€§å·®
"""
        try:
            response = self.client.generate(evaluation_prompt)
            score_match = re.search(r'0?\.\d+|[01]', response)
            quality_score = max(0.0, min(1.0, float(score_match.group()))) if score_match else 0.2
            self.colored_logger.debug(f"ğŸ“Š æ•´ä½“RAGè´¨é‡è¯„ä¼°: {quality_score:.3f}")
            return quality_score
        except Exception as e:
            self.colored_logger.error(f"æ•´ä½“è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            return 0.1

    def _reflect(self, state: ReActState, current_quality: float) -> bool:
        """åæ€é˜¶æ®µ"""
        if current_quality >= self.quality_threshold:
            self.colored_logger.reflection(f"è´¨é‡åˆ† {current_quality:.2f} è¾¾æ ‡, åœæ­¢ã€‚")
            return False
        if state.iteration >= self.max_iterations:
            self.colored_logger.reflection(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°, åœæ­¢ã€‚")
            return False
        if len(state.quality_scores) >= 2 and all(s < 0.3 for s in state.quality_scores[-2:]):
            self.colored_logger.reflection("è´¨é‡åˆ†æŒç»­è¿‡ä½, æå‰åœæ­¢ã€‚")
            return False
        return True

    def _synthesize_retrieved_results(self, section_context: Dict[str, str], state: ReActState) -> Dict[str, List]:
        """åˆæˆæœ€ç»ˆç»“æœä¸ºä¸‰ä¸ªåˆ†ç¦»çš„å­—æ®µ"""
        if not state.retrieved_results:
            return {
                'retrieved_text': [],
                'retrieved_image': [],
                'retrieved_table': []
            }
        
        # æŒ‰ç±»å‹åˆ†ç»„ç»“æœ
        retrieved_text = []
        retrieved_image = []
        retrieved_table = []
        retrieved_web = []  # æ–°å¢Webæœç´¢ç»“æœåˆ†ç»„
        
        for result in state.retrieved_results:
            result_type = result.get('type', 'text')
            if result_type == 'text':
                retrieved_text.append(result)
            elif result_type == 'image':
                retrieved_image.append(result)
            elif result_type == 'table':
                retrieved_table.append(result)
            elif result_type == 'web_text':
                retrieved_web.append(result)
            else:
                # é»˜è®¤å½’ç±»ä¸ºæ–‡æœ¬
                retrieved_text.append(result)
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œæ˜¾ç¤ºåˆ†ç»„ç»“æœ
        self.colored_logger.debug(f"ğŸ“Š åˆ†ç»„ç»“æœ: æ–‡æœ¬{len(retrieved_text)}æ¡, å›¾ç‰‡{len(retrieved_image)}æ¡, è¡¨æ ¼{len(retrieved_table)}æ¡, Web{len(retrieved_web)}æ¡")
        
        # æ˜¾ç¤ºå›¾ç‰‡ç»“æœçš„è¯¦ç»†ä¿¡æ¯
        for i, img in enumerate(retrieved_image):
            self.colored_logger.debug(f"ğŸ“¸ å›¾ç‰‡{i+1}: è·¯å¾„={img.get('path', 'N/A')}, é¡µæ•°={img.get('page_number', 'N/A')}, æè¿°={img.get('description', 'N/A')[:50]}...")
        
        # å¤šç»´åº¦æŸ¥è¯¢æ¨¡å¼ï¼šè¿›è¡Œæ™ºèƒ½å»é‡å¤„ç†
        retrieved_text = self._deduplicate_results(retrieved_text, 'text')
        retrieved_image = self._deduplicate_results(retrieved_image, 'image') 
        retrieved_table = self._deduplicate_results(retrieved_table, 'table')
        retrieved_web = self._deduplicate_results(retrieved_web, 'web_text')
        
        self.colored_logger.debug(f"ğŸ“Š å»é‡åç»“æœ: æ–‡æœ¬{len(retrieved_text)}æ¡, å›¾ç‰‡{len(retrieved_image)}æ¡, è¡¨æ ¼{len(retrieved_table)}æ¡, Web{len(retrieved_web)}æ¡")
        
        # åˆ†æ®µæœç´¢ç»“æœç»Ÿè®¡
        self.colored_logger.observation(f"ğŸ“Š æœ€ç»ˆç»“æœç»Ÿè®¡: æ–‡æœ¬{len(retrieved_text)}æ¡, "
                                      f"å›¾ç‰‡{len(retrieved_image)}æ¡, "
                                      f"è¡¨æ ¼{len(retrieved_table)}æ¡, "
                                      f"Web{len(retrieved_web)}æ¡")

        # ç¡®ä¿å›¾ç‰‡ç»“æœåŒ…å«å®Œæ•´çš„è·¯å¾„å’Œæè¿°ä¿¡æ¯
        final_image_results = []
        for img_result in retrieved_image:
            # ä¿ç•™æ‰€æœ‰é‡è¦çš„å›¾ç‰‡ä¿¡æ¯
            final_img = {
                'content': img_result.get('content', ''),
                'source': img_result.get('source', 'å¤–éƒ¨API'),
                'type': 'image',
                'path': img_result.get('path', ''),
                'page_number': img_result.get('page_number', ''),
                'description': img_result.get('description', ''),
                'score': img_result.get('score', 1.0),
                # ä¿ç•™è¯¦ç»†æè¿°å’Œå·¥ç¨‹æŠ€æœ¯ä¿¡æ¯
                'detailed_description': img_result.get('detailed_description', ''),
                'engineering_details': img_result.get('engineering_details', '')
            }
            final_image_results.append(final_img)
            
            # æ·»åŠ è°ƒè¯•æ—¥å¿—
            self.colored_logger.debug(f"ğŸ“¸ æœ€ç»ˆå›¾ç‰‡ç»“æœ: è·¯å¾„={final_img['path']}, æè¿°={final_img['description']}, é¡µæ•°={final_img['page_number']}, è¯¦ç»†æè¿°={final_img['detailed_description'][:50]}...")
        
        # ç¡®ä¿è¡¨æ ¼ç»“æœåŒ…å«é¡µæ•°ä¿¡æ¯
        final_table_results = []
        for table_result in retrieved_table:
            final_table_results.append({
                'content': table_result.get('content', ''),
                'source': table_result.get('source', 'å¤–éƒ¨API'),
                'type': 'table',
                'page_number': table_result.get('page_number', ''),
                'score': table_result.get('score', 1.0)
            })
        
        # ç¡®ä¿æ–‡æœ¬ç»“æœåŒ…å«é¡µæ•°ä¿¡æ¯
        final_text_results = []
        for text_result in retrieved_text:
            final_text_results.append({
                'content': text_result.get('content', ''),
                'source': text_result.get('source', 'å¤–éƒ¨API'),
                'type': 'text',
                'page_number': text_result.get('page_number', ''),
                'score': text_result.get('score', 1.0)
            })

        # å¤„ç†Webæœç´¢ç»“æœ
        final_web_results = []
        for web_result in retrieved_web:
            final_web_results.append({
                'content': web_result.get('content', ''),
                'source': web_result.get('source', 'Webæœç´¢'),
                'type': 'web_text',
                'url': web_result.get('url', ''),
                'title': web_result.get('title', ''),
                'score': web_result.get('score', 1.0)
            })

        return {
            'retrieved_text': final_text_results,
            'retrieved_image': final_image_results,
            'retrieved_table': final_table_results,
            'retrieved_web': final_web_results
        }

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gauzæ–‡æ¡£Agent - FastAPIæœåŠ¡å™¨
å°†å¤šAgentæ–‡æ¡£ç”Ÿæˆç³»ç»Ÿå°è£…ä¸ºRESTful APIæœåŠ¡

æä¾›çš„æ¥å£ï¼š
- POST /generate_document - ç”Ÿæˆæ–‡æ¡£ï¼ˆè‡ªåŠ¨ç®¡ç†è¾“å‡ºç›®å½•ï¼‰
- GET /health - å¥åº·æ£€æŸ¥
- GET /status - ç³»ç»ŸçŠ¶æ€
- POST /set_concurrency - è®¾ç½®å¹¶å‘å‚æ•°
- GET /download/{file_id} - ä¸‹è½½ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆå¤‡ç”¨ï¼‰
- MinIOè‡ªåŠ¨ä¸Šä¼  - ä¸»è¦æ–‡ä»¶åˆ†å‘æ–¹å¼
"""

import sys
import os
import uuid
import asyncio
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path

# å¿…é¡»åœ¨æ‰€æœ‰å…¶ä»–å¯¼å…¥ä¹‹å‰ç¦ç”¨ChromaDB telemetry
os.environ['ANONYMIZED_TELEMETRY'] = 'False'
os.environ['CHROMA_TELEMETRY_DISABLED'] = 'True'

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from fastapi import FastAPI, HTTPException, BackgroundTasks, status
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn
import json
import re
import time
import threading
from contextvars import ContextVar

# å¯¼å…¥ä¸»è¦ç»„ä»¶
try:
    from main import DocumentGenerationPipeline
    from config.settings import setup_logging, get_config
    from config.minio_config import get_minio_client, upload_document_files
    from one_click_pipeline import one_click_generate_document
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

# è®¾ç½®æ—¥å¿—
setup_logging()
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="Gauzæ–‡æ¡£Agent API",
    description="åŸºäºå¤šAgentæ¶æ„çš„æ™ºèƒ½é•¿æ–‡æ¡£ç”Ÿæˆç³»ç»ŸAPIæœåŠ¡",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
pipeline: Optional[DocumentGenerationPipeline] = None
generation_tasks: Dict[str, Dict[str, Any]] = {}  # å­˜å‚¨ä»»åŠ¡çŠ¶æ€
file_storage: Dict[str, str] = {}  # å­˜å‚¨æ–‡ä»¶æ˜ å°„

# ===== æ—¥å¿—æ¡¥æ¥åˆ°SSEï¼ˆæŒ‰ä»»åŠ¡ï¼‰ =====

# çº¿ç¨‹ID â†’ ä»»åŠ¡ID æ˜ å°„ï¼ˆå°†åå°æ‰§è¡Œçº¿ç¨‹äº§ç”Ÿçš„æ—¥å¿—ç»‘å®šåˆ°å½“å‰ä»»åŠ¡ï¼‰
_thread_task_map: Dict[int, str] = {}
# æ­£åœ¨é€šè¿‡SSEæµå¼ä¼ è¾“çš„ä»»åŠ¡é›†åˆï¼šç”¨äºé¿å…ä¸å…¨å±€æ¡¥æ¥å™¨é‡å¤æ¨é€
_active_sse_tasks: set[str] = set()
# ä»»åŠ¡SSEé€‰é¡¹ï¼ˆä¾‹å¦‚æ˜¯å¦è¯¦ç»†è¾“å‡ºï¼‰
_task_stream_options: Dict[str, Dict[str, Any]] = {}

# éverboseæ¨¡å¼ä¸‹æŠ‘åˆ¶çš„æ ‡å‡†æ—¥å¿—ç‰‡æ®µ
_SSE_SUPPRESSED_PATTERNS = [
    "åˆå§‹åŒ–å®Œæˆ",
    "æ–‡æ¡£ç”Ÿæˆæ™ºèƒ½é€Ÿç‡æ§åˆ¶å™¨åˆå§‹åŒ–",
    "å·²è·³è¿‡Webæœç´¢å¥åº·æ£€æŸ¥",
    "Sending request to OpenRouter",
    "Token usage:",
    "OpenRouter APIè°ƒç”¨æˆåŠŸ",
    "OpenRouterå®¢æˆ·ç«¯ä¼šè¯å·²å…³é—­",
]

class _StdIOTee:
    """Duplicate writes to original stream and push per-line to task SSE logs based on threadâ†’taskæ˜ å°„ã€‚"""
    def __init__(self, original_stream, source: str):
        self._original = original_stream
        self._source = source  # 'stdout' or 'stderr'
        self._buffer = ''
        self._lock = threading.Lock()

    def write(self, data):
        if not isinstance(data, str):
            data = str(data)
        with self._lock:
            try:
                self._original.write(data)
                self._original.flush()
            except Exception:
                pass
            self._buffer += data
            while '\n' in self._buffer:
                line, self._buffer = self._buffer.split('\n', 1)
                try:
                    task_id = _thread_task_map.get(threading.get_ident())
                    if task_id and line.strip() != '':
                        log_manager.add_log(task_id, {
                            'type': 'info',
                            'message': line,
                            'source': self._source,
                            'sse_only': True
                        })
                except Exception:
                    pass

    def flush(self):
        try:
            self._original.flush()
        except Exception:
            pass

class TaskLogHandler(logging.Handler):
    """å°†æ ‡å‡†æ—¥å¿—è·¯ç”±åˆ°å¯¹åº”ä»»åŠ¡çš„SSEæ—¥å¿—é˜Ÿåˆ—ã€‚"""
    def emit(self, record: logging.LogRecord):
        try:
            # é¿å…é€’å½’ï¼šå¿½ç•¥æœ¬æ¨¡å—ä¸uvicornæ—¥å¿—
            if record.name in ("api_server", "uvicorn", "uvicorn.error", "uvicorn.access"):
                return
            task_id = _thread_task_map.get(getattr(record, 'thread', None))
            if not task_id:
                return
            # è‹¥è¯¥ä»»åŠ¡æ­£åœ¨é€šè¿‡SSEæµå¼è¾“å‡ºï¼Œåˆ™ç”±TaskScopedHandlerè´Ÿè´£æ¨é€ï¼Œè¿™é‡Œé¿å…é‡å¤
            if task_id in _active_sse_tasks:
                return
            level = record.levelname.lower()
            log_type = 'error' if level == 'error' else ('warning' if level == 'warning' else 'info')
            log_entry = {
                'type': log_type,
                'message': record.getMessage(),
                'logger': record.name,
            }
            # ç›´æ¥å†™å…¥ä»»åŠ¡æ—¥å¿—ï¼ˆå†…éƒ¨ä¼šå†å†™ç³»ç»Ÿæ—¥å¿—ï¼Œä½†æˆ‘ä»¬å·²å±è”½api_serverï¼Œé¿å…å›ç¯ï¼‰
            log_manager.add_log(task_id, log_entry)
        except Exception:
            # é¿å…SSEå› æ—¥å¿—å¤„ç†å¼‚å¸¸è€Œä¸­æ–­
            pass

class TaskScopedHandler(logging.Handler):
    """å°†æ‰€æœ‰æ—¥å¿—(å…¨å±€)è·¯ç”±åˆ°æŒ‡å®štask_idå¯¹åº”çš„SSEï¼Œä¸å†å†™å›ç³»ç»Ÿloggerï¼Œé¿å…å›ç¯ã€‚"""
    def __init__(self, task_id: str):
        super().__init__()
        self.task_id = task_id
    def emit(self, record: logging.LogRecord):
        try:
            if record.name in ("uvicorn", "uvicorn.error", "uvicorn.access"):
                return
            # è®°å½•å·²è¢«æŸä¸ªåŒtaskçš„å¤„ç†å™¨è·¯ç”±è¿‡ï¼Œé¿å…é‡å¤
            routed_task = getattr(record, '_sse_routed_task_id', None)
            if routed_task == self.task_id:
                return
            try:
                setattr(record, '_sse_routed_task_id', self.task_id)
            except Exception:
                pass
            # éverboseæ¨¡å¼æŠ‘åˆ¶éƒ¨åˆ†å†—ä½™åˆå§‹åŒ–/è®¡è´¹æ—¥å¿—
            opts = _task_stream_options.get(self.task_id, {})
            verbose = bool(opts.get('verbose', False))
            message_text = record.getMessage()
            if not verbose:
                for frag in _SSE_SUPPRESSED_PATTERNS:
                    if frag in message_text:
                        return
            level = record.levelname.lower()
            log_type = 'error' if level == 'error' else ('warning' if level == 'warning' else 'info')
            log_entry = {
                'type': log_type,
                'message': message_text,
                'logger': record.name,
                'sse_only': True,
            }
            log_manager.add_log(self.task_id, log_entry)
        except Exception:
            pass

# åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œçš„åŒ…è£…å™¨ï¼šç¡®ä¿çº¿ç¨‹â†’ä»»åŠ¡IDæ˜ å°„å­˜åœ¨ï¼Œä¾¿äºè·¯ç”±æ—¥å¿—
def _wrapped_generate_without_eval(task_id: str, query: str, project_name: str, output_dir: str):
    try:
        _thread_task_map[threading.get_ident()] = task_id
        return pipeline.generate_document_without_evaluation(query, project_name, output_dir)
    finally:
        _thread_task_map.pop(threading.get_ident(), None)

def _wrapped_one_click(task_id: str, query: str, project_name: str, output_dir: str, enable_review_and_regeneration: bool):
    try:
        _thread_task_map[threading.get_ident()] = task_id
        return one_click_generate_document(query, project_name, output_dir, enable_review_and_regeneration)
    finally:
        _thread_task_map.pop(threading.get_ident(), None)

# ===== æ—¥å¿—ç®¡ç†å™¨ =====

class LogManager:
    """ä»»åŠ¡æ—¥å¿—ç®¡ç†å™¨"""
    def __init__(self):
        self.task_logs: Dict[str, List[Dict[str, Any]]] = {}  # å­˜å‚¨ä»»åŠ¡æ—¥å¿—
        # è®¢é˜…è€…ä¿¡æ¯ï¼š{ task_id: [ { 'queue': asyncio.Queue, 'loop': asyncio.AbstractEventLoop } ] }
        self.log_subscribers: Dict[str, List[Dict[str, Any]]] = {}
        self.max_logs_per_task = 1000  # æ¯ä¸ªä»»åŠ¡æœ€å¤šä¿å­˜çš„æ—¥å¿—æ•°é‡
        self.loop: Optional[asyncio.AbstractEventLoop] = None  # ä¸»äº‹ä»¶å¾ªç¯ï¼ˆç”¨äºè·¨çº¿ç¨‹å®‰å…¨æ¨é€ï¼‰
        
    def add_log(self, task_id: str, log_entry: Dict[str, Any]):
        """æ·»åŠ æ—¥å¿—æ¡ç›®"""
        if task_id not in self.task_logs:
            self.task_logs[task_id] = []
        
        # æ·»åŠ æ—¶é—´æˆ³ï¼ˆå¦‚æœæ²¡æœ‰çš„è¯ï¼‰
        if 'timestamp' not in log_entry:
            log_entry['timestamp'] = datetime.now().isoformat()
        
        # ç»Ÿä¸€å»é™¤ANSIé¢œè‰²ç ï¼Œé˜²æ­¢å‰ç«¯æ˜¾ç¤ºå¼‚å¸¸
        try:
            msg = str(log_entry.get('message', ''))
            msg = re.sub(r"\x1b\[[0-9;]*[A-Za-z]", "", msg)
            log_entry['message'] = msg
        except Exception:
            pass

        # å»é‡ï¼šä¸ä¸Šä¸€æ¡å®Œå…¨ç›¸åŒåˆ™è·³è¿‡
        try:
            last_entry = self.task_logs[task_id][-1] if self.task_logs[task_id] else None
            if (
                last_entry
                and last_entry.get('message') == log_entry.get('message')
                and last_entry.get('logger') == log_entry.get('logger')
                and last_entry.get('type') == log_entry.get('type')
            ):
                # è·³è¿‡é‡å¤
                return
        except Exception:
            pass

        self.task_logs[task_id].append(log_entry)
        
        # é™åˆ¶æ—¥å¿—æ•°é‡ï¼Œé¿å…å†…å­˜æº¢å‡º
        if len(self.task_logs[task_id]) > self.max_logs_per_task:
            self.task_logs[task_id] = self.task_logs[task_id][-self.max_logs_per_task:]
        
        # æ¨é€ç»™æ‰€æœ‰è®¢é˜…è€…
        self._notify_subscribers(task_id, log_entry)
        
        # åŒæ—¶è®°å½•åˆ°ç³»ç»Ÿæ—¥å¿—ï¼ˆé¿å…é€’å½’ï¼šæ ‡è®°ä¸ºsse_onlyçš„ä¸å†å†™å›ç³»ç»Ÿæ—¥å¿—ï¼‰
        if not log_entry.get('sse_only'):
            log_level = log_entry.get('type', 'info')
            message = f"[{task_id}] {log_entry.get('message', '')}"
            if log_level == 'error':
                logger.error(message)
            elif log_level == 'warning':
                logger.warning(message)
            else:
                logger.info(message)
    
    def _notify_subscribers(self, task_id: str, log_entry: Dict[str, Any]):
        """é€šçŸ¥è®¢é˜…è€…ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰ï¼šä½¿ç”¨å„è‡ªçš„äº‹ä»¶å¾ªç¯è°ƒåº¦å†™å…¥é˜Ÿåˆ—"""
        if task_id not in self.log_subscribers:
            return

        subscribers = list(self.log_subscribers[task_id])

        def _queue_put_safe(q: asyncio.Queue, entry: Dict[str, Any]):
            try:
                q.put_nowait(entry)
            except asyncio.QueueFull:
                dropped = 0
                try:
                    while dropped < 10:
                        q.get_nowait()
                        dropped += 1
                    q.put_nowait(entry)
                except Exception:
                    pass

        for sub in subscribers:
            try:
                target_loop = sub.get('loop')
                target_queue = sub.get('queue')
                if target_loop and target_loop.is_running():
                    target_loop.call_soon_threadsafe(_queue_put_safe, target_queue, log_entry)
                elif self.loop and self.loop.is_running():
                    # å›é€€ï¼šå°è¯•åœ¨ä¸»å¾ªç¯è°ƒåº¦ï¼ˆåŒå¾ªç¯æ—¶æœ‰æ•ˆï¼‰
                    try:
                        self.loop.call_soon_threadsafe(_queue_put_safe, target_queue, log_entry)
                    except Exception:
                        pass
                else:
                    # æœ€åé€€åŒ–ï¼šç›´æ¥è°ƒç”¨ï¼ˆä»…åœ¨åŒçº¿ç¨‹/æ— äº‹ä»¶å¾ªç¯æ—¶ï¼‰
                    _queue_put_safe(target_queue, log_entry)
            except Exception:
                pass
    
    async def subscribe_logs(self, task_id: str) -> asyncio.Queue:
        """è®¢é˜…ä»»åŠ¡æ—¥å¿—ï¼ˆè®°å½•è®¢é˜…è€…äº‹ä»¶å¾ªç¯ï¼‰"""
        queue = asyncio.Queue(maxsize=1000)
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = None
        if task_id not in self.log_subscribers:
            self.log_subscribers[task_id] = []
        self.log_subscribers[task_id].append({'queue': queue, 'loop': loop})
        return queue
    
    def unsubscribe_logs(self, task_id: str, queue: asyncio.Queue):
        """å–æ¶ˆè®¢é˜…ä»»åŠ¡æ—¥å¿—"""
        if task_id in self.log_subscribers:
            try:
                remaining: List[Dict[str, Any]] = []
                for sub in self.log_subscribers[task_id]:
                    if sub.get('queue') is not queue:
                        remaining.append(sub)
                if remaining:
                    self.log_subscribers[task_id] = remaining
                else:
                    del self.log_subscribers[task_id]
            except Exception:
                pass
    
    def get_logs(self, task_id: str) -> List[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çš„æ‰€æœ‰æ—¥å¿—"""
        return self.task_logs.get(task_id, [])
    
    def cleanup_task_logs(self, task_id: str):
        """æ¸…ç†ä»»åŠ¡æ—¥å¿—ï¼ˆä»»åŠ¡å®Œæˆåè°ƒç”¨ï¼‰"""
        # ä¿ç•™æ—¥å¿—1å°æ—¶ï¼Œç„¶åæ¸…ç†
        if task_id in self.task_logs:
            # è¿™é‡Œå¯ä»¥å®ç°å»¶æ—¶æ¸…ç†ï¼Œæš‚æ—¶ä¿ç•™
            pass
        
        # ç«‹å³æ¸…ç†è®¢é˜…è€…
        if task_id in self.log_subscribers:
            del self.log_subscribers[task_id]

# åˆ›å»ºå…¨å±€æ—¥å¿—ç®¡ç†å™¨
log_manager = LogManager()

# ===== æ•°æ®æ¨¡å‹ =====

class DocumentGenerationRequest(BaseModel):
    """æ–‡æ¡£ç”Ÿæˆè¯·æ±‚æ¨¡å‹"""
    query: str = Field(..., description="æ–‡æ¡£ç”Ÿæˆéœ€æ±‚æè¿°", min_length=1, max_length=2000)
    project_name: str = Field(..., description="é¡¹ç›®åç§°ï¼Œç”¨äºRAGæ£€ç´¢", min_length=1, max_length=100)
    
    class Config:
        json_schema_extra = {
            "example": {
                "query": "æˆ‘æƒ³ç”Ÿæˆä¸€ä¸ªå…³äºåŒ»çµå¤åº™çš„æ–‡ç‰©å½±å“è¯„ä¼°æŠ¥å‘Š",
                "project_name": "åŒ»çµå¤åº™"
            }
        }

class OneClickGenerationRequest(BaseModel):
    """ä¸€é”®ä¸²è”å·¥ä½œæµè¯·æ±‚æ¨¡å‹ï¼ˆç»“æ„â†’æ£€ç´¢â†’æˆæ–‡â†’è¯„å®¡â†’å†ç”Ÿâ†’åˆå¹¶ï¼‰"""
    query: str = Field(..., description="æ–‡æ¡£ç”Ÿæˆéœ€æ±‚æè¿°", min_length=1, max_length=2000)
    project_name: str = Field(..., description="é¡¹ç›®åç§°ï¼Œç”¨äºRAGæ£€ç´¢", min_length=1, max_length=100)
    enable_review_and_regeneration: bool = Field(default=False, description="æ˜¯å¦å¯ç”¨è¯„å®¡+å†ç”Ÿ+åˆå¹¶")

    class Config:
        json_schema_extra = {
            "example": {
                "query": "æˆ‘æƒ³ç”Ÿæˆä¸€ä¸ªå…³äºåŒ»çµå¤åº™çš„æ–‡ç‰©å½±å“è¯„ä¼°æŠ¥å‘Š",
                "project_name": "åŒ»çµå¤åº™",
                "enable_review_and_regeneration": False
            }
        }

class ConcurrencySettings(BaseModel):
    """å¹¶å‘è®¾ç½®æ¨¡å‹"""
    orchestrator_workers: Optional[int] = Field(None, ge=1, le=10, description="ç¼–æ’ä»£ç†çº¿ç¨‹æ•°")
    react_workers: Optional[int] = Field(None, ge=1, le=10, description="æ£€ç´¢ä»£ç†çº¿ç¨‹æ•°")
    content_workers: Optional[int] = Field(None, ge=1, le=10, description="å†…å®¹ç”Ÿæˆä»£ç†çº¿ç¨‹æ•°")
    rate_delay: Optional[float] = Field(None, ge=0.1, le=10.0, description="è¯·æ±‚é—´éš”æ—¶é—´(ç§’)")
    
    class Config:
        json_schema_extra = {
            "example": {
                "orchestrator_workers": 3,
                "react_workers": 5,
                "content_workers": 4,
                "rate_delay": 1.0
            }
        }

class TaskStatus(BaseModel):
    """ä»»åŠ¡çŠ¶æ€æ¨¡å‹"""
    task_id: str
    status: str  # pending, running, completed, failed
    progress: str
    created_at: datetime
    updated_at: datetime
    request: Optional[Dict[str, Any]] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class DocumentGenerationResponse(BaseModel):
    """æ–‡æ¡£ç”Ÿæˆå“åº”æ¨¡å‹"""
    task_id: str = Field(..., description="ä»»åŠ¡ID")
    status: str = Field(..., description="ä»»åŠ¡çŠ¶æ€")
    message: str = Field(..., description="å“åº”æ¶ˆæ¯")
    files: Optional[Dict[str, str]] = Field(None, description="ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆæœ¬åœ°ä¸‹è½½é“¾æ¥ï¼‰")
    minio_urls: Optional[Dict[str, str]] = Field(None, description="MinIOå­˜å‚¨çš„æ–‡ä»¶ä¸‹è½½é“¾æ¥")

class SystemStatus(BaseModel):
    """ç³»ç»ŸçŠ¶æ€æ¨¡å‹"""
    service: str
    status: str
    version: str
    active_tasks: int
    total_tasks: int
    concurrency_settings: Dict[str, Any]
    uptime: str
    minio_status: str = Field(..., description="MinIOå­˜å‚¨æœåŠ¡çŠ¶æ€")

# ===== åˆå§‹åŒ–å‡½æ•° =====

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    global pipeline
    try:
        logger.info("ğŸš€ æ­£åœ¨å¯åŠ¨Gauzæ–‡æ¡£Agent APIæœåŠ¡...")
        pipeline = DocumentGenerationPipeline()
        logger.info("âœ… æ–‡æ¡£ç”Ÿæˆæµæ°´çº¿åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("outputs", exist_ok=True)
        os.makedirs("api_outputs", exist_ok=True)
        
        # åˆå§‹åŒ–MinIOå®¢æˆ·ç«¯
        minio_client = get_minio_client()
        if minio_client.is_available():
            logger.info("âœ… MinIOå®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
        else:
            logger.warning("âš ï¸ MinIOå®¢æˆ·ç«¯è¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ–‡ä»¶å­˜å‚¨")
        
        logger.info("ğŸŒŸ Gauzæ–‡æ¡£Agent APIæœåŠ¡å¯åŠ¨å®Œæˆï¼")
        # è®°å½•äº‹ä»¶å¾ªç¯åˆ°æ—¥å¿—ç®¡ç†å™¨ï¼Œä¾¿äºè·¨çº¿ç¨‹å®‰å…¨æ¨é€SSE
        try:
            log_manager.loop = asyncio.get_event_loop()
        except Exception:
            pass

        # å®‰è£…æ—¥å¿—æ¡¥æ¥å¤„ç†å™¨ï¼ˆä¸€æ¬¡ï¼‰
        try:
            bridge_installed = any(isinstance(h, TaskLogHandler) for h in logging.getLogger().handlers)
            if not bridge_installed:
                logging.getLogger().addHandler(TaskLogHandler())
                logger.info("âœ… å·²å¯ç”¨ä»»åŠ¡æ—¥å¿—æ¡¥æ¥åˆ°SSE")
        except Exception as e:
            logger.warning(f"âš ï¸ å¯ç”¨æ—¥å¿—æ¡¥æ¥å¤±è´¥: {e}")
        
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """å…³é—­æ—¶æ¸…ç†èµ„æº"""
    logger.info("ğŸ”„ æ­£åœ¨å…³é—­Gauzæ–‡æ¡£Agent APIæœåŠ¡...")
    
    # æ¸…ç†æœªå®Œæˆçš„ä»»åŠ¡
    for task_id, task_info in generation_tasks.items():
        if task_info["status"] in ["pending", "running"]:
            task_info["status"] = "cancelled"
            task_info["updated_at"] = datetime.now()
    
    logger.info("âœ… æœåŠ¡å…³é—­å®Œæˆ")

# ===== æ ¸å¿ƒAPIæ¥å£ =====

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "healthy",
        "service": "Gauzæ–‡æ¡£Agent API",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0"
    }

@app.get("/logs/{task_id}/stream")
async def stream_task_logs(task_id: str):
    """å®æ—¶æ¨é€ä»»åŠ¡æ—¥å¿—æµï¼ˆServer-Sent Eventsï¼‰"""
    
    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
    if task_id not in generation_tasks:
        raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
    
    async def log_generator():
        """æ—¥å¿—ç”Ÿæˆå™¨"""
        log_queue = None
        try:
            # è®¢é˜…æ—¥å¿—
            log_queue = await log_manager.subscribe_logs(task_id)
            
            # é¦–å…ˆå‘é€å†å²æ—¥å¿—
            historical_logs = log_manager.get_logs(task_id)
            for log_entry in historical_logs:
                data = json.dumps(log_entry, ensure_ascii=False)
                yield f"data: {data}\n\n"
            
            # å‘é€å½“å‰ä»»åŠ¡çŠ¶æ€
            task_status_log = {
                "timestamp": datetime.now().isoformat(),
                "type": "status",
                "message": f"å½“å‰ä»»åŠ¡çŠ¶æ€: {generation_tasks[task_id]['status']}",
                "task_status": generation_tasks[task_id]['status'],
                "progress": generation_tasks[task_id].get('progress', ''),
            }
            data = json.dumps(task_status_log, ensure_ascii=False)
            yield f"data: {data}\n\n"
            
            # å®æ—¶æ¨é€æ–°æ—¥å¿—
            while True:
                try:
                    # ç­‰å¾…æ–°çš„æ—¥å¿—æ¡ç›®ï¼Œè®¾ç½®è¶…æ—¶é˜²æ­¢è¿æ¥æŒ‚èµ·
                    log_entry = await asyncio.wait_for(log_queue.get(), timeout=30.0)
                    data = json.dumps(log_entry, ensure_ascii=False)
                    yield f"data: {data}\n\n"
                    
                    # ä»…åœ¨ä»»åŠ¡çœŸæ­£å®Œæˆ/å¤±è´¥ï¼Œæˆ–æ”¶åˆ°æ˜¾å¼å®Œæˆä¿¡å·æ—¶ç»“æŸ
                    task_status = generation_tasks.get(task_id, {}).get('status')
                    if task_status in ['completed', 'failed'] or log_entry.get('step') == 'ä»»åŠ¡å®Œæˆ' or log_entry.get('type') == 'success':
                        await asyncio.sleep(1)
                        end_log = {
                            "timestamp": datetime.now().isoformat(),
                            "type": "stream_end",
                            "message": "æ—¥å¿—æµç»“æŸ"
                        }
                        data = json.dumps(end_log, ensure_ascii=False)
                        yield f"data: {data}\n\n"
                        break
                        
                except asyncio.TimeoutError:
                    # å‘é€å¿ƒè·³ï¼Œä¿æŒè¿æ¥æ´»è·ƒ
                    heartbeat = {
                        "timestamp": datetime.now().isoformat(),
                        "type": "heartbeat",
                        "message": "è¿æ¥æ­£å¸¸"
                    }
                    data = json.dumps(heartbeat, ensure_ascii=False)
                    yield f"data: {data}\n\n"
                    # å¿ƒè·³æ—¶ä¹Ÿæ£€æŸ¥ä»»åŠ¡çŠ¶æ€ï¼Œé¿å…å› é”™è¯¯æ—¥å¿—æœªè§¦å‘å®Œæˆè€Œæ‚¬æŒ‚
                    task_status = generation_tasks.get(task_id, {}).get('status')
                    if task_status in ['completed', 'failed']:
                        end_log = {
                            "timestamp": datetime.now().isoformat(),
                            "type": "stream_end",
                            "message": "æ—¥å¿—æµç»“æŸ"
                        }
                        data = json.dumps(end_log, ensure_ascii=False)
                        yield f"data: {data}\n\n"
                        break
                    
        except Exception as e:
            # å‘é€é”™è¯¯ä¿¡æ¯
            error_log = {
                "timestamp": datetime.now().isoformat(),
                "type": "stream_error",
                "message": f"æ—¥å¿—æµå¼‚å¸¸: {str(e)}"
            }
            data = json.dumps(error_log, ensure_ascii=False)
            yield f"data: {data}\n\n"
            
        finally:
            # æ¸…ç†è®¢é˜…
            if log_queue:
                log_manager.unsubscribe_logs(task_id, log_queue)
    
    return StreamingResponse(
        log_generator(),
        media_type="text/event-stream; charset=utf-8",
        headers={
            "Cache-Control": "no-cache, no-transform",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control",
        }
    )

@app.get("/logs/{task_id}")
async def get_task_logs(task_id: str):
    """è·å–ä»»åŠ¡çš„å†å²æ—¥å¿—"""
    
    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
    if task_id not in generation_tasks:
        raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
    
    logs = log_manager.get_logs(task_id)
    
    return {
        "task_id": task_id,
        "task_status": generation_tasks[task_id]["status"],
        "log_count": len(logs),
        "logs": logs,
        "last_updated": generation_tasks[task_id]["updated_at"].isoformat()
    }

@app.get("/status", response_model=SystemStatus)
async def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    active_tasks = sum(1 for task in generation_tasks.values() 
                      if task["status"] in ["pending", "running"])
    
    # è®¡ç®—è¿è¡Œæ—¶é—´ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    uptime = "è¿è¡Œä¸­"
    
    # æ£€æŸ¥MinIOçŠ¶æ€
    minio_client = get_minio_client()
    minio_status = "available" if minio_client.is_available() else "unavailable"
    
    return SystemStatus(
        service="Gauzæ–‡æ¡£Agent API",
        status="running",
        version="1.0.0",
        active_tasks=active_tasks,
        total_tasks=len(generation_tasks),
        concurrency_settings=pipeline.get_concurrency_settings(),
        uptime=uptime,
        minio_status=minio_status
    )

@app.post("/set_concurrency")
async def set_concurrency(settings: ConcurrencySettings):
    """è®¾ç½®å¹¶å‘å‚æ•°"""
    if not pipeline:
        raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
    try:
        pipeline.set_concurrency(
            orchestrator_workers=settings.orchestrator_workers,
            react_workers=settings.react_workers,
            content_workers=settings.content_workers,
            rate_delay=settings.rate_delay
        )
        
        logger.info(f"âœ… å¹¶å‘è®¾ç½®å·²æ›´æ–°: {settings.dict()}")
        
        return {
            "status": "success",
            "message": "å¹¶å‘è®¾ç½®å·²æ›´æ–°",
            "current_settings": pipeline.get_concurrency_settings()
        }
        
    except Exception as e:
        logger.error(f"âŒ è®¾ç½®å¹¶å‘å‚æ•°å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è®¾ç½®å¤±è´¥: {str(e)}")

# @app.post("/generate_document", response_model=DocumentGenerationResponse)
# async def generate_document(request: DocumentGenerationRequest, background_tasks: BackgroundTasks):
#     """
#     ç”Ÿæˆæ–‡æ¡£æ¥å£ - å¼‚æ­¥å¤„ç†
    
#     æäº¤æ–‡æ¡£ç”Ÿæˆä»»åŠ¡ï¼Œè¿”å›ä»»åŠ¡IDã€‚å¯é€šè¿‡ä»»åŠ¡IDæŸ¥è¯¢è¿›åº¦å’Œä¸‹è½½ç»“æœã€‚
#     """
#     if not pipeline:
#         raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
    
#     # ç”Ÿæˆä»»åŠ¡ID
#     task_id = str(uuid.uuid4())
    
#     # åˆ›å»ºä»»åŠ¡è®°å½•
#     task_info = {
#         "task_id": task_id,
#         "status": "pending",
#         "progress": "ä»»åŠ¡å·²æäº¤ï¼Œç­‰å¾…å¤„ç†",
#         "created_at": datetime.now(),
#         "updated_at": datetime.now(),
#         "request": request.dict(),
#         "result": None,
#         "error": None
#     }
    
#     generation_tasks[task_id] = task_info
    
#     # æ·»åŠ åå°ä»»åŠ¡
#     background_tasks.add_task(run_document_generation, task_id, request)
    
#     logger.info(f"ğŸ“ æ–°çš„æ–‡æ¡£ç”Ÿæˆä»»åŠ¡: {task_id} - {request.query}")
    
#     return DocumentGenerationResponse(
#         task_id=task_id,
#         status="pending",
#         message=f"æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å·²æäº¤ï¼Œä»»åŠ¡ID: {task_id}",
#         files=None
    # )

@app.post("/generate_document", response_model=DocumentGenerationResponse)
async def generate_document_full(request: OneClickGenerationRequest, background_tasks: BackgroundTasks):
    """
    ä¸€é”®å¼å®Œæ•´å·¥ä½œæµæ¥å£ï¼ˆç»“æ„â†’æ£€ç´¢â†’æˆæ–‡â†’è¯„å®¡â†’å†ç”Ÿâ†’åˆå¹¶ï¼‰ - å¼‚æ­¥å¤„ç†
    """
    # ç”Ÿæˆä»»åŠ¡ID
    task_id = str(uuid.uuid4())

    # åˆ›å»ºä»»åŠ¡è®°å½•
    task_info = {
        "task_id": task_id,
        "status": "pending",
        "progress": "ä»»åŠ¡å·²æäº¤ï¼Œç­‰å¾…å¤„ç†",
        "created_at": datetime.now(),
        "updated_at": datetime.now(),
        "request": request.dict(),
        "result": None,
        "error": None
    }
    generation_tasks[task_id] = task_info

    # æ·»åŠ åå°ä»»åŠ¡
    background_tasks.add_task(run_one_click_generation, task_id, request)

    logger.info(f"ğŸ“ æ–°çš„å®Œæ•´å·¥ä½œæµä»»åŠ¡: {task_id} - {request.query}")

    return DocumentGenerationResponse(
        task_id=task_id,
        status="pending",
        message=f"å®Œæ•´å·¥ä½œæµä»»åŠ¡å·²æäº¤ï¼Œä»»åŠ¡ID: {task_id}",
        files=None
    )

@app.post("/generate_document/stream")
async def generate_document_stream(request: OneClickGenerationRequest):
    """
    ä»¥SSEå®æ—¶æ¨é€æ—¥å¿—çš„æ–‡æ¡£ç”Ÿæˆæ¥å£ï¼ˆå®Œæ•´å·¥ä½œæµï¼‰ã€‚
    - æäº¤åç«‹å³åˆ›å»ºä»»åŠ¡å¹¶å¯åŠ¨åå°æ‰§è¡Œ
    - åŒä¸€HTTPè¿æ¥ä¸­ä»¥Server-Sent Eventsæ¨é€å†å²ä¸å®æ—¶æ—¥å¿—ï¼Œç›´è‡³ä»»åŠ¡å®Œæˆ
    """
    task_id = str(uuid.uuid4())
    task_info = {
        "task_id": task_id,
        "status": "pending",
        "progress": "ä»»åŠ¡å·²æäº¤ï¼Œç­‰å¾…å¤„ç†",
        "created_at": datetime.now(),
        "updated_at": datetime.now(),
        "request": request.dict(),
        "result": None,
        "error": None
    }
    generation_tasks[task_id] = task_info

    async def event_generator():
        log_queue = None
        # ä¸ºæœ¬ä»»åŠ¡å®‰è£…ä»»åŠ¡çº§æ—¥å¿—å¤„ç†å™¨ï¼Œæ•è·æ‰€æœ‰loggerè¾“å‡ºï¼ˆè·¨çº¿ç¨‹ï¼‰ï¼Œå¹¶åŠ å…¥æ´»åŠ¨é›†åˆé¿å…é‡å¤
        root_logger = logging.getLogger()
        task_handler = TaskScopedHandler(task_id)
        root_logger.addHandler(task_handler)
        _active_sse_tasks.add(task_id)
        # è®°å½•SSEé€‰é¡¹ï¼ˆå½“å‰ä»…æ”¯æŒverboseï¼Œé€šè¿‡æŸ¥è¯¢å‚æ•°ä¼ é€’ï¼‰
        try:
            from fastapi import Request as _FastAPIRequest  # é¿å…é¡¶éƒ¨å¯¼å…¥å†²çª
        except Exception:
            _FastAPIRequest = None
        try:
            # è¯»å–æŸ¥è¯¢å‚æ•° verbose=true/false
            # è¿è¡Œæ—¶ä»fastapiçš„requestå¯¹è±¡å–ï¼ˆè‹¥å‰ç«¯ä¼ é€’äº†ï¼‰
            # è‹¥è·å–å¤±è´¥ï¼Œåˆ™é»˜è®¤False
            verbose_flag = False
            if hasattr(request, '__dict__') and 'query' in request.__dict__:
                # è¿™æ˜¯Pydanticæ¨¡å‹ï¼Œä¸åŒ…å«query params
                pass
            # é€šè¿‡å…¨å±€appä¾èµ–æ³¨å…¥çš„æ–¹å¼ä¸å¯ç”¨ï¼Œè¿™é‡Œé‡‡ç”¨ç¯å¢ƒé»˜è®¤False
            _task_stream_options[task_id] = { 'verbose': verbose_flag }
        except Exception:
            _task_stream_options[task_id] = { 'verbose': False }
        try:
            # è®¢é˜…æ—¥å¿—
            log_queue = await log_manager.subscribe_logs(task_id)

            # å¯åŠ¨åå°å®Œæ•´å·¥ä½œæµ
            asyncio.create_task(run_one_click_generation(task_id, request))

            # é¦–å¸§ï¼šåˆå§‹åŒ–äº‹ä»¶
            init_evt = {
                "type": "init",
                "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œå¼€å§‹æ¨é€æ—¥å¿—",
                "task_id": task_id,
                "query": request.query,
                "project_name": request.project_name
            }
            yield f"data: {json.dumps(init_evt, ensure_ascii=False)}\n\n"

            # æ¨é€å†å²æ—¥å¿—ï¼ˆå¦‚æœæœ‰ï¼‰
            historical_logs = log_manager.get_logs(task_id)
            for log_entry in historical_logs:
                yield f"data: {json.dumps(log_entry, ensure_ascii=False)}\n\n"

            # å®æ—¶æ—¥å¿—
            while True:
                try:
                    log_entry = await asyncio.wait_for(log_queue.get(), timeout=30.0)
                    yield f"data: {json.dumps(log_entry, ensure_ascii=False)}\n\n"
                    if log_entry.get('type') in ['success', 'error'] or log_entry.get('step') == 'ä»»åŠ¡å®Œæˆ':
                        await asyncio.sleep(1)
                        end_evt = {"type": "stream_end", "message": "æ—¥å¿—æµç»“æŸ"}
                        yield f"data: {json.dumps(end_evt, ensure_ascii=False)}\n\n"
                        break
                except asyncio.TimeoutError:
                    heartbeat = {
                        "timestamp": datetime.now().isoformat(),
                        "type": "heartbeat",
                        "message": "è¿æ¥æ­£å¸¸"
                    }
                    yield f"data: {json.dumps(heartbeat, ensure_ascii=False)}\n\n"
        finally:
            if log_queue:
                log_manager.unsubscribe_logs(task_id, log_queue)
            try:
                root_logger.removeHandler(task_handler)
            except Exception:
                pass
            try:
                _active_sse_tasks.discard(task_id)
            except Exception:
                pass
            try:
                _task_stream_options.pop(task_id, None)
            except Exception:
                pass

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream; charset=utf-8",
        headers={
            "Cache-Control": "no-cache, no-transform",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control",
        }
    )

@app.get("/tasks/{task_id}", response_model=TaskStatus)
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in generation_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task_info = generation_tasks[task_id]
    
    return TaskStatus(
        task_id=task_info["task_id"],
        status=task_info["status"],
        progress=task_info["progress"],
        created_at=task_info["created_at"],
        updated_at=task_info["updated_at"],
        request=task_info.get("request"),
        result=task_info["result"],
        error=task_info["error"]
    )

@app.get("/tasks")
async def list_tasks(limit: int = 20, status_filter: Optional[str] = None):
    """è·å–ä»»åŠ¡åˆ—è¡¨"""
    tasks = list(generation_tasks.values())
    
    # çŠ¶æ€è¿‡æ»¤
    if status_filter:
        tasks = [task for task in tasks if task["status"] == status_filter]
    
    # æŒ‰æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
    tasks.sort(key=lambda x: x["created_at"], reverse=True)
    
    # é™åˆ¶æ•°é‡
    tasks = tasks[:limit]
    
    return {
        "total": len(generation_tasks),
        "filtered": len(tasks),
        "tasks": tasks
    }

@app.get("/download/{file_id}")
async def download_file(file_id: str):
    """ä¸‹è½½ç”Ÿæˆçš„æ–‡ä»¶"""
    if file_id not in file_storage:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    file_path = file_storage[file_id]
    
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="æ–‡ä»¶å·²è¢«åˆ é™¤")
    
    filename = os.path.basename(file_path)
    
    return FileResponse(
        path=file_path,
        filename=filename,
        media_type='application/octet-stream'
    )

# ===== åå°ä»»åŠ¡å‡½æ•° =====

async def run_document_generation(task_id: str, request: DocumentGenerationRequest):
    """åå°æ‰§è¡Œæ–‡æ¡£ç”Ÿæˆä»»åŠ¡"""
    task_info = generation_tasks[task_id]
    # å°†å½“å‰çº¿ç¨‹ç»‘å®šåˆ°ä»»åŠ¡ï¼Œç”¨äºæ—¥å¿—æ¡¥æ¥
    try:
        _thread_task_map[threading.get_ident()] = task_id
    except Exception:
        pass
    
    try:
        # æ¨é€å¼€å§‹æ—¥å¿—
        log_manager.add_log(task_id, {
            "type": "info",
            "message": "æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å·²å¯åŠ¨",
            "progress": 0,
            "step": "ä»»åŠ¡åˆå§‹åŒ–",
            "query": request.query,
            "project_name": request.project_name
        })
        
        # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
        task_info["status"] = "running"
        task_info["progress"] = "æ­£åœ¨ç”Ÿæˆæ–‡æ¡£ç»“æ„..."
        task_info["updated_at"] = datetime.now()
        
        # æ¨é€çŠ¶æ€æ›´æ–°
        log_manager.add_log(task_id, {
            "type": "progress",
            "message": "æ­£åœ¨åˆå§‹åŒ–æ–‡æ¡£ç”Ÿæˆæµæ°´çº¿...",
            "progress": 5,
            "step": "æµæ°´çº¿åˆå§‹åŒ–"
        })
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ–‡æ¡£ç”Ÿæˆä»»åŠ¡: {task_id}")
        
        # åˆ›å»ºä»»åŠ¡ä¸“ç”¨è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = f"api_outputs/{task_id}_{timestamp}"
        
        # æ¨é€ç›®å½•åˆ›å»ºæ—¥å¿—
        log_manager.add_log(task_id, {
            "type": "progress",
            "message": f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}",
            "progress": 10,
            "step": "ç›®å½•åˆ›å»º",
            "output_dir": output_dir
        })
        
        # æ¨é€æ–‡æ¡£ç”Ÿæˆå¼€å§‹
        log_manager.add_log(task_id, {
            "type": "progress",
            "message": "å¼€å§‹æ‰§è¡Œå¤šAgentæ–‡æ¡£ç”Ÿæˆæµæ°´çº¿...",
            "progress": 15,
            "step": "å¤šAgentåä½œ"
        })
        
        # åœ¨æ–°çš„çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥ä»£ç ï¼ˆAPIæ¨¡å¼ï¼Œè·³è¿‡è´¨é‡è¯„ä¼°ï¼‰
        loop = asyncio.get_event_loop()
        result_files = await loop.run_in_executor(
            None,
            _wrapped_generate_without_eval,
            task_id,
            request.query,
            request.project_name,
            output_dir
        )
        
        # æ¨é€æ–‡æ¡£ç”Ÿæˆå®Œæˆ
        log_manager.add_log(task_id, {
            "type": "progress",
            "message": "æ–‡æ¡£å†…å®¹ç”Ÿæˆå®Œæˆï¼Œæ­£åœ¨å¤„ç†æ–‡ä»¶...",
            "progress": 70,
            "step": "æ–‡ä»¶å¤„ç†",
            "generated_files": list(result_files.keys())
        })
        
        # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶ä¸‹è½½é“¾æ¥
        file_links = {}
        for file_type, file_path in result_files.items():
            if file_type != 'output_directory' and os.path.exists(file_path):
                file_id = str(uuid.uuid4())
                file_storage[file_id] = file_path
                file_links[file_type] = f"/download/{file_id}"
        
        # ä¸Šä¼ æ–‡ä»¶åˆ°MinIO
        task_info["progress"] = "æ­£åœ¨ä¸Šä¼ æ–‡ä»¶åˆ°MinIO..."
        task_info["updated_at"] = datetime.now()
        
        # æ¨é€MinIOä¸Šä¼ å¼€å§‹
        log_manager.add_log(task_id, {
            "type": "progress",
            "message": "å¼€å§‹ä¸Šä¼ æ–‡ä»¶åˆ°äº‘å­˜å‚¨(MinIO)...",
            "progress": 80,
            "step": "äº‘å­˜å‚¨ä¸Šä¼ "
        })
        
        minio_urls = {}
        try:
            logger.info(f"ğŸ“¤ å¼€å§‹ä¸Šä¼ æ–‡ä»¶åˆ°MinIO: {task_id}")
            minio_urls = upload_document_files(result_files, task_id)
            if minio_urls:
                logger.info(f"âœ… MinIOä¸Šä¼ æˆåŠŸ: {len(minio_urls)} ä¸ªæ–‡ä»¶")
                log_manager.add_log(task_id, {
                    "type": "success",
                    "message": f"äº‘å­˜å‚¨ä¸Šä¼ æˆåŠŸï¼Œå…±ä¸Šä¼  {len(minio_urls)} ä¸ªæ–‡ä»¶",
                    "progress": 90,
                    "step": "ä¸Šä¼ å®Œæˆ",
                    "minio_files": len(minio_urls)
                })
            else:
                logger.warning(f"âš ï¸ MinIOä¸Šä¼ å¤±è´¥ï¼Œä»…æä¾›æœ¬åœ°ä¸‹è½½")
                log_manager.add_log(task_id, {
                    "type": "warning",
                    "message": "äº‘å­˜å‚¨ä¸Šä¼ å¤±è´¥ï¼Œä»…æä¾›æœ¬åœ°ä¸‹è½½",
                    "progress": 85,
                    "step": "ä¸Šä¼ å¤±è´¥"
                })
        except Exception as e:
            logger.error(f"âŒ MinIOä¸Šä¼ å¼‚å¸¸: {e}")
            log_manager.add_log(task_id, {
                "type": "error",
                "message": f"äº‘å­˜å‚¨ä¸Šä¼ å¼‚å¸¸: {str(e)}",
                "progress": 85,
                "step": "ä¸Šä¼ å¼‚å¸¸",
                "error": str(e)
            })
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        task_info["status"] = "completed"
        task_info["progress"] = "æ–‡æ¡£ç”Ÿæˆå’Œä¸Šä¼ å®Œæˆ"
        task_info["result"] = {
            "files": file_links,
            "minio_urls": minio_urls,
            "output_directory": result_files.get("output_directory"),
            "generation_time": datetime.now().isoformat(),
            "storage_info": {
                "local_files": len(file_links),
                "minio_files": len(minio_urls),
                "total_size_mb": sum(
                    os.path.getsize(file_path) / (1024 * 1024) 
                    for file_path in result_files.values() 
                    if file_path != result_files.get("output_directory") and os.path.exists(file_path)
                )
            }
        }
        task_info["updated_at"] = datetime.now()
        
        # æ¨é€ä»»åŠ¡å®Œæˆæ—¥å¿—
        log_manager.add_log(task_id, {
            "type": "success",
            "message": "âœ… æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å®Œæˆï¼",
            "progress": 100,
            "step": "ä»»åŠ¡å®Œæˆ",
            "result": {
                "minio_urls": minio_urls,
                "local_files": file_links,
                "storage_info": task_info["result"]["storage_info"]
            }
        })
        
        logger.info(f"âœ… æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å®Œæˆ: {task_id}")
        
    except Exception as e:
        # æ¨é€é”™è¯¯æ—¥å¿—
        log_manager.add_log(task_id, {
            "type": "error",
            "message": f"âŒ æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å¤±è´¥: {str(e)}",
            "progress": 0,
            "step": "ä»»åŠ¡å¤±è´¥",
            "error": str(e)
        })
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        task_info["status"] = "failed"
        task_info["progress"] = f"ç”Ÿæˆå¤±è´¥: {str(e)}"
        task_info["error"] = str(e)
        task_info["updated_at"] = datetime.now()
        
        logger.error(f"âŒ æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å¤±è´¥: {task_id} - {e}")
    finally:
        # ä»»åŠ¡å®Œæˆåæ¸…ç†æ—¥å¿—è®¢é˜…è€…ï¼ˆä½†ä¿ç•™æ—¥å¿—1å°æ—¶ï¼‰
        try:
            _thread_task_map.pop(threading.get_ident(), None)
        except Exception:
            pass
        log_manager.cleanup_task_logs(task_id)


async def run_one_click_generation(task_id: str, request: OneClickGenerationRequest):
    """åå°æ‰§è¡Œä¸€é”®å·¥ä½œæµä»»åŠ¡ï¼ˆåŒ…å«è¯„å®¡/å†ç”Ÿ/åˆå¹¶ï¼‰"""
    task_info = generation_tasks[task_id]
    # å°†å½“å‰çº¿ç¨‹ç»‘å®šåˆ°ä»»åŠ¡ï¼Œç”¨äºæ—¥å¿—æ¡¥æ¥
    try:
        _thread_task_map[threading.get_ident()] = task_id
    except Exception:
        pass
    try:
        # å°†stdout/stderr teeåˆ°ä»»åŠ¡SSE
        original_stdout, original_stderr = sys.stdout, sys.stderr
        sys.stdout = _StdIOTee(original_stdout, 'stdout')
        sys.stderr = _StdIOTee(original_stderr, 'stderr')
        # å¯åŠ¨æ—¥å¿—
        log_manager.add_log(task_id, {
            "type": "info",
            "message": "å®Œæ•´å·¥ä½œæµä»»åŠ¡å·²å¯åŠ¨",
            "progress": 0,
            "step": "ä»»åŠ¡åˆå§‹åŒ–",
            "query": request.query,
            "project_name": request.project_name,
            "enable_review_and_regeneration": request.enable_review_and_regeneration,
        })

        task_info["status"] = "running"
        task_info["progress"] = "æ­£åœ¨æ‰§è¡Œå®Œæ•´å·¥ä½œæµ..."
        task_info["updated_at"] = datetime.now()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = f"api_outputs/{task_id}_{timestamp}"
        os.makedirs(output_dir, exist_ok=True)

        log_manager.add_log(task_id, {
            "type": "progress",
            "message": f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}",
            "progress": 10,
            "step": "ç›®å½•åˆ›å»º",
            "output_dir": output_dir
        })

        # æ‰§è¡Œä¸€é”®å·¥ä½œæµï¼ˆåŒæ­¥è½¬çº¿ç¨‹ï¼‰
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            _wrapped_one_click,
            task_id,
            request.query,
            request.project_name,
            output_dir,
            request.enable_review_and_regeneration,
        )

        # æ•´ç†äº§ç‰©
        log_manager.add_log(task_id, {
            "type": "progress",
            "message": "å·¥ä½œæµå®Œæˆï¼Œæ­£åœ¨å¤„ç†äº§ç‰©...",
            "progress": 75,
            "step": "æ–‡ä»¶å¤„ç†",
        })

        # å½’é›†ä¸»è¦æ–‡ä»¶
        files_to_publish: Dict[str, str] = {}
        try:
            final_md = result.get("final_document")
            if final_md and os.path.exists(final_md):
                files_to_publish["final_markdown"] = final_md

            stages = result.get("stages", {})
            # ç»“æ„/æ£€ç´¢/æˆæ–‡
            if stages.get("structure_and_guides", {}).get("file"):
                files_to_publish["step1_guide_json"] = stages["structure_and_guides"]["file"]
            if stages.get("retrieval_enrichment", {}).get("file"):
                files_to_publish["step2_enriched_json"] = stages["retrieval_enrichment"]["file"]
            if stages.get("content_generation", {}).get("json"):
                files_to_publish["generated_json"] = stages["content_generation"]["json"]
            if stages.get("content_generation", {}).get("markdown"):
                files_to_publish["generated_markdown"] = stages["content_generation"]["markdown"]
            # è¯„å®¡
            if stages.get("quality_review", {}).get("issues_file"):
                files_to_publish["quality_issues_json"] = stages["quality_review"]["issues_file"]
            # å†ç”Ÿ/åˆå¹¶
            if stages.get("regeneration", {}).get("file"):
                files_to_publish["regenerated_sections_json"] = stages["regeneration"]["file"]
            if stages.get("merge_and_render", {}).get("merged_json"):
                files_to_publish["merged_json"] = stages["merge_and_render"]["merged_json"]
            if stages.get("merge_and_render", {}).get("merged_markdown"):
                files_to_publish["merged_markdown"] = stages["merge_and_render"]["merged_markdown"]
            if stages.get("merge_and_render", {}).get("summary"):
                files_to_publish["merge_summary_md"] = stages["merge_and_render"]["summary"]
        except Exception:
            pass

        # ç”Ÿæˆæœ¬åœ°ä¸‹è½½é“¾æ¥
        file_links = {}
        for file_type, file_path in files_to_publish.items():
            if os.path.exists(file_path):
                file_id = str(uuid.uuid4())
                file_storage[file_id] = file_path
                file_links[file_type] = f"/download/{file_id}"

        # ä¸Šä¼ åˆ°MinIO
        task_info["progress"] = "æ­£åœ¨ä¸Šä¼ æ–‡ä»¶åˆ°MinIO..."
        task_info["updated_at"] = datetime.now()
        log_manager.add_log(task_id, {
            "type": "progress",
            "message": "å¼€å§‹ä¸Šä¼ æ–‡ä»¶åˆ°äº‘å­˜å‚¨(MinIO)...",
            "progress": 85,
            "step": "äº‘å­˜å‚¨ä¸Šä¼ "
        })

        minio_urls = upload_document_files(files_to_publish, task_id)

        # å®Œæˆ
        task_info["status"] = "completed"
        task_info["progress"] = "å®Œæ•´å·¥ä½œæµå®Œæˆ"
        task_info["result"] = {
            "files": file_links,
            "minio_urls": minio_urls,
            "output_directory": result.get("output_directory"),
            "stages": result.get("stages"),
            "final_document": result.get("final_document"),
        }
        task_info["updated_at"] = datetime.now()

        log_manager.add_log(task_id, {
            "type": "success",
            "message": "âœ… å®Œæ•´å·¥ä½œæµä»»åŠ¡å®Œæˆï¼",
            "progress": 100,
            "step": "ä»»åŠ¡å®Œæˆ",
            "result": {
                "minio_urls": minio_urls,
                "local_files": file_links,
                "final_document": result.get("final_document"),
            }
        })

    except Exception as e:
        log_manager.add_log(task_id, {
            "type": "error",
            "message": f"âŒ å®Œæ•´å·¥ä½œæµä»»åŠ¡å¤±è´¥: {str(e)}",
            "progress": 0,
            "step": "ä»»åŠ¡å¤±è´¥",
            "error": str(e)
        })
        task_info["status"] = "failed"
        task_info["progress"] = f"ç”Ÿæˆå¤±è´¥: {str(e)}"
        task_info["error"] = str(e)
        task_info["updated_at"] = datetime.now()
        logger.error(f"âŒ å®Œæ•´å·¥ä½œæµä»»åŠ¡å¤±è´¥: {task_id} - {e}")
    finally:
        try:
            _thread_task_map.pop(threading.get_ident(), None)
        except Exception:
            pass
        # æ¢å¤stdout/stderr
        try:
            sys.stdout = original_stdout
            sys.stderr = original_stderr
        except Exception:
            pass
        log_manager.cleanup_task_logs(task_id)

# ===== å­—æ®µæœç´¢æ¥å£ =====

class FieldSearchRequest(BaseModel):
    """å­—æ®µæœç´¢è¯·æ±‚æ¨¡å‹"""
    query: str = Field(..., description="æœç´¢æŸ¥è¯¢æ–‡æœ¬", min_length=1, max_length=1000)
    project_name: str = Field(..., description="é¡¹ç›®åç§°", min_length=1, max_length=100)
    search_type: str = Field(default="hybrid", description="æœç´¢ç±»å‹ï¼ˆhybrid/vector/bm25ï¼‰")
    initial_top_k: int = Field(default=20, ge=1, le=100, description="åˆæ­¥æ£€ç´¢è¿”å›çš„ç»“æœæ•°é‡")
    final_top_k: int = Field(default=10, ge=1, le=50, description="é‡æ’åºåæœ€ç»ˆè¿”å›çš„ç»“æœæ•°é‡")
    chunk_type: Optional[str] = Field(default=None, description="æŒ‡å®šæœç´¢çš„å­—æ®µç±»å‹ï¼špage_text/detailed_description/engineering_details/None")

    class Config:
        json_schema_extra = {
            "example": {
                "query": "åœ°ç†ä½ç½®",
                "project_name": "åŒ»çµå¤åº™",
                "search_type": "hybrid",
                "initial_top_k": 20,
                "final_top_k": 10,
                "chunk_type": "page_text"
            }
        }

class FieldSearchResponse(BaseModel):
    """å­—æ®µæœç´¢å“åº”æ¨¡å‹"""
    success: bool = Field(..., description="æœç´¢æ˜¯å¦æˆåŠŸ")
    message: str = Field(..., description="å“åº”æ¶ˆæ¯")
    data: Optional[Dict[str, Any]] = Field(None, description="æœç´¢ç»“æœæ•°æ®")
    search_params: Dict[str, Any] = Field(..., description="æœç´¢å‚æ•°")
    processing_time: float = Field(..., description="å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰")

# @app.post("/api/v1/search_by_field", response_model=FieldSearchResponse)
# async def search_by_field(request: FieldSearchRequest):
#     """
#     æŒ‰å­—æ®µç±»å‹åˆ†å¼€å¬å›æœç´¢æ¥å£
    
#     æ”¯æŒæŒ‰å­—æ®µç±»å‹åˆ†å¼€å¬å›æ–‡æœ¬å’Œå›¾ç‰‡å†…å®¹ï¼Œæ”¯æŒé‡æ’åº
#     """
#     start_time = time.time()
    
#     try:
#         logger.info(f"ğŸ” å­—æ®µæœç´¢è¯·æ±‚: {request.query} (é¡¹ç›®: {request.project_name}, å­—æ®µç±»å‹: {request.chunk_type})")
        
#         # éªŒè¯å­—æ®µç±»å‹
#         valid_chunk_types = ["page_text", "detailed_description", "engineering_details", None]
#         if request.chunk_type not in valid_chunk_types:
#             raise HTTPException(
#                 status_code=400,
#                 detail=f"æ— æ•ˆçš„å­—æ®µç±»å‹: {request.chunk_type}ã€‚æœ‰æ•ˆç±»å‹: {valid_chunk_types}"
#             )
        
#         # æ ¹æ®å­—æ®µç±»å‹æ„é€ ä¸åŒçš„æœç´¢ç­–ç•¥
#         search_strategy = {
#             "text_search": {
#                 "chunk_type": "page_text",
#                 "description": "é¡µé¢æ–‡æœ¬å†…å®¹æœç´¢"
#             },
#             "image_search": {
#                 "chunk_type": "detailed_description", 
#                 "description": "å›¾ç‰‡è¯¦ç»†æè¿°æœç´¢"
#             },
#             "engineering_search": {
#                 "chunk_type": "engineering_details",
#                 "description": "å·¥ç¨‹æŠ€æœ¯ç»†èŠ‚æœç´¢"
#             }
#         }
        
#         # æ ¹æ®chunk_typeç¡®å®šæœç´¢ç­–ç•¥
#         if request.chunk_type == "page_text":
#             current_strategy = search_strategy["text_search"]
#         elif request.chunk_type == "detailed_description":
#             current_strategy = search_strategy["image_search"]
#         elif request.chunk_type == "engineering_details":
#             current_strategy = search_strategy["engineering_search"]
#         else:
#             # å¦‚æœæœªæŒ‡å®šå­—æ®µç±»å‹ï¼Œè¿”å›æ‰€æœ‰ç±»å‹çš„æœç´¢ç»“æœ
#             current_strategy = None
        
#         # æ¨¡æ‹Ÿæœç´¢ç»“æœï¼ˆå®é™…å®ç°ä¸­è¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„RAGæ£€ç´¢æœåŠ¡ï¼‰
#         mock_results = {
#             "page_text_results": [],
#             "detailed_description_results": [],
#             "engineering_details_results": [],
#             "search_metadata": {
#                 "query": request.query,
#                 "project_name": request.project_name,
#                 "search_type": request.search_type,
#                 "initial_top_k": request.initial_top_k,
#                 "final_top_k": request.final_top_k,
#                 "chunk_type": request.chunk_type,
#                 "strategy": current_strategy["description"] if current_strategy else "å…¨å­—æ®µæœç´¢"
#             }
#         }
        
#         # æ ¹æ®å­—æ®µç±»å‹ç”Ÿæˆç›¸åº”çš„æ¨¡æ‹Ÿæ•°æ®
#         if request.chunk_type == "page_text" or request.chunk_type is None:
#             mock_results["page_text_results"] = [
#                 {
#                     "page_number": 1,
#                     "content": f"å…³äº{request.query}çš„è¯¦ç»†æ–‡æœ¬æè¿°...",
#                     "similarity": 0.95,
#                     "rerank_score": 0.92,
#                     "images": ["image1.jpg", "image2.jpg"]
#                 },
#                 {
#                     "page_number": 2,
#                     "content": f"{request.query}ç›¸å…³çš„å†å²èƒŒæ™¯ä¿¡æ¯...",
#                     "similarity": 0.88,
#                     "rerank_score": 0.85,
#                     "images": ["image3.jpg"]
#                 }
#             ]
        
#         if request.chunk_type == "detailed_description" or request.chunk_type is None:
#             mock_results["detailed_description_results"] = [
#                 {
#                     "image_url": "image1.jpg",
#                     "detailed_description": f"å›¾ç‰‡å±•ç¤ºäº†{request.query}çš„è¯¦ç»†ç‰¹å¾...",
#                     "similarity": 0.93,
#                     "rerank_score": 0.90,
#                     "page_number": 1
#                 },
#                 {
#                     "image_url": "image2.jpg", 
#                     "detailed_description": f"è¯¥å›¾ç‰‡æè¿°äº†{request.query}çš„å…·ä½“ç»†èŠ‚...",
#                     "similarity": 0.87,
#                     "rerank_score": 0.84,
#                     "page_number": 1
#                 }
#             ]
        
#         if request.chunk_type == "engineering_details" or request.chunk_type is None:
#             mock_results["engineering_details_results"] = [
#                 {
#                     "image_url": "image1.jpg",
#                     "engineering_details": f"{request.query}çš„å·¥ç¨‹æŠ€æœ¯å‚æ•°å’Œè§„æ ¼...",
#                     "similarity": 0.91,
#                     "rerank_score": 0.89,
#                     "page_number": 1
#                 }
#             ]
        
#         processing_time = time.time() - start_time
        
#         logger.info(f"âœ… å­—æ®µæœç´¢æˆåŠŸ: è€—æ—¶ {processing_time:.2f}s")
        
#         return FieldSearchResponse(
#             success=True,
#             message="å­—æ®µæœç´¢æˆåŠŸ",
#             data=mock_results,
#             search_params={
#                 "query": request.query,
#                 "project_name": request.project_name,
#                 "search_type": request.search_type,
#                 "initial_top_k": request.initial_top_k,
#                 "final_top_k": request.final_top_k,
#                 "chunk_type": request.chunk_type
#             },
#             processing_time=processing_time
#         )
        
#     except HTTPException:
#         raise
#     except Exception as e:
#         processing_time = time.time() - start_time
#         logger.error(f"âŒ å­—æ®µæœç´¢å¤±è´¥: {e}")
        
#         return FieldSearchResponse(
#             success=False,
#             message=f"å­—æ®µæœç´¢å¤±è´¥: {str(e)}",
#             data=None,
#             search_params={
#                 "query": request.query,
#                 "project_name": request.project_name,
#                 "search_type": request.search_type,
#                 "initial_top_k": request.initial_top_k,
#                 "final_top_k": request.final_top_k,
#                 "chunk_type": request.chunk_type
#             },
#             processing_time=processing_time
#         )

# ===== å¯åŠ¨æœåŠ¡å™¨ =====

def start_server(host: str = "0.0.0.0", port: int = 8000, reload: bool = False):
    """å¯åŠ¨FastAPIæœåŠ¡å™¨"""
    print("ğŸš€ å¯åŠ¨Gauzæ–‡æ¡£Agent APIæœåŠ¡å™¨...")
    print(f"ğŸ“Š æœåŠ¡åœ°å€: http://{host}:{port}")
    print(f"ğŸ“– APIæ–‡æ¡£: http://{host}:{port}/docs")
    print(f"ğŸ“š ReDocæ–‡æ¡£: http://{host}:{port}/redoc")
    
    uvicorn.run(
        "api_server:app",
        host=host,
        port=port,
        reload=reload,
        log_level="info"
    )

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Gauzæ–‡æ¡£Agent APIæœåŠ¡å™¨")
    parser.add_argument("--host", default="0.0.0.0", help="æœåŠ¡å™¨ä¸»æœºåœ°å€")
    parser.add_argument("--port", type=int, default=8002, help="æœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--reload", action="store_true", help="å¼€å‘æ¨¡å¼è‡ªåŠ¨é‡è½½")
    
    args = parser.parse_args()
    start_server(args.host, args.port, args.reload)